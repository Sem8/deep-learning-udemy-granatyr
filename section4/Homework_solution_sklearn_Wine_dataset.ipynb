{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sem8/deep-learning-udemy-granatyr/blob/main/Homework_solution_sklearn_Wine_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFopIDwcj4eR"
      },
      "source": [
        "# Homework solution - sklearn 1 (wine classification)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmwQCDnBmJil"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "http://archive.ics.uci.edu/ml/datasets/Wine/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui7djQN6_oxN"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT6atVUN_2qh"
      },
      "source": [
        "wine = datasets.load_wine()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlgWzsdq_-N9",
        "outputId": "ae276a1b-a97b-4c6e-ad45-169ceb352fc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "inputs = wine.data\n",
        "inputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
              "        1.065e+03],\n",
              "       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n",
              "        1.050e+03],\n",
              "       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n",
              "        1.185e+03],\n",
              "       ...,\n",
              "       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
              "        8.350e+02],\n",
              "       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
              "        8.400e+02],\n",
              "       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
              "        5.600e+02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJsmHbR9AJQx",
        "outputId": "5a4ac8af-0624-48a4-c6a1-318d669c5777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "wine.feature_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alcohol',\n",
              " 'malic_acid',\n",
              " 'ash',\n",
              " 'alcalinity_of_ash',\n",
              " 'magnesium',\n",
              " 'total_phenols',\n",
              " 'flavanoids',\n",
              " 'nonflavanoid_phenols',\n",
              " 'proanthocyanins',\n",
              " 'color_intensity',\n",
              " 'hue',\n",
              " 'od280/od315_of_diluted_wines',\n",
              " 'proline']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywNWxtC7APmQ",
        "outputId": "2f291c10-56fd-4027-fa58-091e9b562209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "outputs = wine.target\n",
        "outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw-qvoPWAcjm",
        "outputId": "30361d92-0975-49e4-d51b-337c876430bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "wine.target_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['class_0', 'class_1', 'class_2'], dtype='<U7')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SyaocNLAgiZ",
        "outputId": "31fabec8-688d-45cf-da96-cb498ee2d6f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "inputs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7mhrjwxAkmW",
        "outputId": "4ca21ea8-b280-43cc-f2d6-23a740342273",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "outputs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF29OkpYmMsV"
      },
      "source": [
        "## Train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGXWZc5fBaEI"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BZ631mgB71t",
        "outputId": "4d1fb0e8-769c-485e-ac88-ab1a9a6e59e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(142, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxIE6aWOCBug",
        "outputId": "42581444-9200-47d9-b3ad-2ada4b4e7ed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(142,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz9VXQm_CLrV",
        "outputId": "0284a14c-8071-486c-cc97-c2425f1d7037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic-v5HmMCVGD",
        "outputId": "23a9ce40-2225-4c32-c96f-1ab4f428b4d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S5gJUwomQ3U"
      },
      "source": [
        "## Neural network (training)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duOEv22KMHLE",
        "outputId": "05d9a646-aac7-4d1c-b701-430c51d732c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(13 + 3) / 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E22nm4baD8eO",
        "outputId": "7e6b24ae-75b2-4e57-a5ca-e93327483a10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 13 -> 8 -> 8 -> 3\n",
        "network = MLPClassifier(max_iter=2000,\n",
        "                        verbose=True,\n",
        "                        activation = 'logistic',\n",
        "                        solver = 'adam',\n",
        "                        learning_rate = 'constant',\n",
        "                        learning_rate_init = 0.001,\n",
        "                        tol=0.00000100,\n",
        "                        hidden_layer_sizes = (8, 8))\n",
        "network.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.10483804\n",
            "Iteration 2, loss = 1.10358508\n",
            "Iteration 3, loss = 1.10225655\n",
            "Iteration 4, loss = 1.10084997\n",
            "Iteration 5, loss = 1.09939724\n",
            "Iteration 6, loss = 1.09793026\n",
            "Iteration 7, loss = 1.09645554\n",
            "Iteration 8, loss = 1.09498482\n",
            "Iteration 9, loss = 1.09361011\n",
            "Iteration 10, loss = 1.09243575\n",
            "Iteration 11, loss = 1.09148991\n",
            "Iteration 12, loss = 1.09071632\n",
            "Iteration 13, loss = 1.09000002\n",
            "Iteration 14, loss = 1.08925307\n",
            "Iteration 15, loss = 1.08845267\n",
            "Iteration 16, loss = 1.08760756\n",
            "Iteration 17, loss = 1.08673718\n",
            "Iteration 18, loss = 1.08586910\n",
            "Iteration 19, loss = 1.08503734\n",
            "Iteration 20, loss = 1.08427284\n",
            "Iteration 21, loss = 1.08359136\n",
            "Iteration 22, loss = 1.08298967\n",
            "Iteration 23, loss = 1.08244916\n",
            "Iteration 24, loss = 1.08194285\n",
            "Iteration 25, loss = 1.08144545\n",
            "Iteration 26, loss = 1.08094236\n",
            "Iteration 27, loss = 1.08043273\n",
            "Iteration 28, loss = 1.07992676\n",
            "Iteration 29, loss = 1.07943994\n",
            "Iteration 30, loss = 1.07898674\n",
            "Iteration 31, loss = 1.07857581\n",
            "Iteration 32, loss = 1.07820780\n",
            "Iteration 33, loss = 1.07787560\n",
            "Iteration 34, loss = 1.07756696\n",
            "Iteration 35, loss = 1.07726929\n",
            "Iteration 36, loss = 1.07697460\n",
            "Iteration 37, loss = 1.07668187\n",
            "Iteration 38, loss = 1.07639608\n",
            "Iteration 39, loss = 1.07612498\n",
            "Iteration 40, loss = 1.07587501\n",
            "Iteration 41, loss = 1.07564832\n",
            "Iteration 42, loss = 1.07544202\n",
            "Iteration 43, loss = 1.07524985\n",
            "Iteration 44, loss = 1.07506493\n",
            "Iteration 45, loss = 1.07488256\n",
            "Iteration 46, loss = 1.07470162\n",
            "Iteration 47, loss = 1.07452420\n",
            "Iteration 48, loss = 1.07435389\n",
            "Iteration 49, loss = 1.07419364\n",
            "Iteration 50, loss = 1.07404411\n",
            "Iteration 51, loss = 1.07390317\n",
            "Iteration 52, loss = 1.07376695\n",
            "Iteration 53, loss = 1.07363171\n",
            "Iteration 54, loss = 1.07349566\n",
            "Iteration 55, loss = 1.07335948\n",
            "Iteration 56, loss = 1.07322527\n",
            "Iteration 57, loss = 1.07309494\n",
            "Iteration 58, loss = 1.07296885\n",
            "Iteration 59, loss = 1.07284564\n",
            "Iteration 60, loss = 1.07272312\n",
            "Iteration 61, loss = 1.07259947\n",
            "Iteration 62, loss = 1.07247413\n",
            "Iteration 63, loss = 1.07234779\n",
            "Iteration 64, loss = 1.07222170\n",
            "Iteration 65, loss = 1.07209675\n",
            "Iteration 66, loss = 1.07197280\n",
            "Iteration 67, loss = 1.07184885\n",
            "Iteration 68, loss = 1.07172369\n",
            "Iteration 69, loss = 1.07159678\n",
            "Iteration 70, loss = 1.07146842\n",
            "Iteration 71, loss = 1.07133946\n",
            "Iteration 72, loss = 1.07121051\n",
            "Iteration 73, loss = 1.07108157\n",
            "Iteration 74, loss = 1.07095208\n",
            "Iteration 75, loss = 1.07082142\n",
            "Iteration 76, loss = 1.07068936\n",
            "Iteration 77, loss = 1.07055619\n",
            "Iteration 78, loss = 1.07042242\n",
            "Iteration 79, loss = 1.07028836\n",
            "Iteration 80, loss = 1.07015394\n",
            "Iteration 81, loss = 1.07001880\n",
            "Iteration 82, loss = 1.06988265\n",
            "Iteration 83, loss = 1.06974550\n",
            "Iteration 84, loss = 1.06960762\n",
            "Iteration 85, loss = 1.06946929\n",
            "Iteration 86, loss = 1.06933056\n",
            "Iteration 87, loss = 1.06919127\n",
            "Iteration 88, loss = 1.06905120\n",
            "Iteration 89, loss = 1.06891031\n",
            "Iteration 90, loss = 1.06876872\n",
            "Iteration 91, loss = 1.06862660\n",
            "Iteration 92, loss = 1.06848403\n",
            "Iteration 93, loss = 1.06834091\n",
            "Iteration 94, loss = 1.06819710\n",
            "Iteration 95, loss = 1.06805256\n",
            "Iteration 96, loss = 1.06790737\n",
            "Iteration 97, loss = 1.06776161\n",
            "Iteration 98, loss = 1.06761531\n",
            "Iteration 99, loss = 1.06746842\n",
            "Iteration 100, loss = 1.06732084\n",
            "Iteration 101, loss = 1.06717256\n",
            "Iteration 102, loss = 1.06702361\n",
            "Iteration 103, loss = 1.06687406\n",
            "Iteration 104, loss = 1.06672389\n",
            "Iteration 105, loss = 1.06657306\n",
            "Iteration 106, loss = 1.06642152\n",
            "Iteration 107, loss = 1.06626926\n",
            "Iteration 108, loss = 1.06611632\n",
            "Iteration 109, loss = 1.06596272\n",
            "Iteration 110, loss = 1.06580843\n",
            "Iteration 111, loss = 1.06565342\n",
            "Iteration 112, loss = 1.06549766\n",
            "Iteration 113, loss = 1.06534116\n",
            "Iteration 114, loss = 1.06518395\n",
            "Iteration 115, loss = 1.06502601\n",
            "Iteration 116, loss = 1.06486733\n",
            "Iteration 117, loss = 1.06470787\n",
            "Iteration 118, loss = 1.06454765\n",
            "Iteration 119, loss = 1.06438665\n",
            "Iteration 120, loss = 1.06422490\n",
            "Iteration 121, loss = 1.06406236\n",
            "Iteration 122, loss = 1.06389903\n",
            "Iteration 123, loss = 1.06373489\n",
            "Iteration 124, loss = 1.06356995\n",
            "Iteration 125, loss = 1.06340421\n",
            "Iteration 126, loss = 1.06323765\n",
            "Iteration 127, loss = 1.06307026\n",
            "Iteration 128, loss = 1.06290204\n",
            "Iteration 129, loss = 1.06273298\n",
            "Iteration 130, loss = 1.06256308\n",
            "Iteration 131, loss = 1.06239232\n",
            "Iteration 132, loss = 1.06222071\n",
            "Iteration 133, loss = 1.06204822\n",
            "Iteration 134, loss = 1.06187486\n",
            "Iteration 135, loss = 1.06170062\n",
            "Iteration 136, loss = 1.06152548\n",
            "Iteration 137, loss = 1.06134945\n",
            "Iteration 138, loss = 1.06117252\n",
            "Iteration 139, loss = 1.06099467\n",
            "Iteration 140, loss = 1.06081589\n",
            "Iteration 141, loss = 1.06063620\n",
            "Iteration 142, loss = 1.06045556\n",
            "Iteration 143, loss = 1.06027397\n",
            "Iteration 144, loss = 1.06009142\n",
            "Iteration 145, loss = 1.05990791\n",
            "Iteration 146, loss = 1.05972339\n",
            "Iteration 147, loss = 1.05953784\n",
            "Iteration 148, loss = 1.05935116\n",
            "Iteration 149, loss = 1.05916315\n",
            "Iteration 150, loss = 1.05897331\n",
            "Iteration 151, loss = 1.05878045\n",
            "Iteration 152, loss = 1.05858162\n",
            "Iteration 153, loss = 1.05836936\n",
            "Iteration 154, loss = 1.05812497\n",
            "Iteration 155, loss = 1.05780251\n",
            "Iteration 156, loss = 1.05730051\n",
            "Iteration 157, loss = 1.05648380\n",
            "Iteration 158, loss = 1.05588879\n",
            "Iteration 159, loss = 1.05565710\n",
            "Iteration 160, loss = 1.05476357\n",
            "Iteration 161, loss = 1.05412730\n",
            "Iteration 162, loss = 1.05343169\n",
            "Iteration 163, loss = 1.05231264\n",
            "Iteration 164, loss = 1.05089548\n",
            "Iteration 165, loss = 1.04973264\n",
            "Iteration 166, loss = 1.04912062\n",
            "Iteration 167, loss = 1.04863790\n",
            "Iteration 168, loss = 1.04796476\n",
            "Iteration 169, loss = 1.04711224\n",
            "Iteration 170, loss = 1.04606066\n",
            "Iteration 171, loss = 1.04506181\n",
            "Iteration 172, loss = 1.04439211\n",
            "Iteration 173, loss = 1.04385272\n",
            "Iteration 174, loss = 1.04313008\n",
            "Iteration 175, loss = 1.04228042\n",
            "Iteration 176, loss = 1.04141758\n",
            "Iteration 177, loss = 1.04056745\n",
            "Iteration 178, loss = 1.03976184\n",
            "Iteration 179, loss = 1.03902542\n",
            "Iteration 180, loss = 1.03833165\n",
            "Iteration 181, loss = 1.03759145\n",
            "Iteration 182, loss = 1.03676902\n",
            "Iteration 183, loss = 1.03592737\n",
            "Iteration 184, loss = 1.03511587\n",
            "Iteration 185, loss = 1.03432752\n",
            "Iteration 186, loss = 1.03354738\n",
            "Iteration 187, loss = 1.03277369\n",
            "Iteration 188, loss = 1.03199458\n",
            "Iteration 189, loss = 1.03118410\n",
            "Iteration 190, loss = 1.03034245\n",
            "Iteration 191, loss = 1.02950795\n",
            "Iteration 192, loss = 1.02870959\n",
            "Iteration 193, loss = 1.02792740\n",
            "Iteration 194, loss = 1.02711842\n",
            "Iteration 195, loss = 1.02627551\n",
            "Iteration 196, loss = 1.02543130\n",
            "Iteration 197, loss = 1.02460892\n",
            "Iteration 198, loss = 1.02379656\n",
            "Iteration 199, loss = 1.02297118\n",
            "Iteration 200, loss = 1.02212596\n",
            "Iteration 201, loss = 1.02127408\n",
            "Iteration 202, loss = 1.02043286\n",
            "Iteration 203, loss = 1.01959991\n",
            "Iteration 204, loss = 1.01875536\n",
            "Iteration 205, loss = 1.01789343\n",
            "Iteration 206, loss = 1.01703038\n",
            "Iteration 207, loss = 1.01617645\n",
            "Iteration 208, loss = 1.01532044\n",
            "Iteration 209, loss = 1.01444985\n",
            "Iteration 210, loss = 1.01357065\n",
            "Iteration 211, loss = 1.01269560\n",
            "Iteration 212, loss = 1.01182196\n",
            "Iteration 213, loss = 1.01093722\n",
            "Iteration 214, loss = 1.01004224\n",
            "Iteration 215, loss = 1.00914765\n",
            "Iteration 216, loss = 1.00825333\n",
            "Iteration 217, loss = 1.00735025\n",
            "Iteration 218, loss = 1.00643777\n",
            "Iteration 219, loss = 1.00552326\n",
            "Iteration 220, loss = 1.00460734\n",
            "Iteration 221, loss = 1.00368340\n",
            "Iteration 222, loss = 1.00275107\n",
            "Iteration 223, loss = 1.00181574\n",
            "Iteration 224, loss = 1.00087707\n",
            "Iteration 225, loss = 0.99993066\n",
            "Iteration 226, loss = 0.99897709\n",
            "Iteration 227, loss = 0.99801978\n",
            "Iteration 228, loss = 0.99705766\n",
            "Iteration 229, loss = 0.99608795\n",
            "Iteration 230, loss = 0.99511198\n",
            "Iteration 231, loss = 0.99413158\n",
            "Iteration 232, loss = 0.99314527\n",
            "Iteration 233, loss = 0.99215196\n",
            "Iteration 234, loss = 0.99115300\n",
            "Iteration 235, loss = 0.99014886\n",
            "Iteration 236, loss = 0.98913829\n",
            "Iteration 237, loss = 0.98812130\n",
            "Iteration 238, loss = 0.98709881\n",
            "Iteration 239, loss = 0.98607053\n",
            "Iteration 240, loss = 0.98503592\n",
            "Iteration 241, loss = 0.98399546\n",
            "Iteration 242, loss = 0.98294933\n",
            "Iteration 243, loss = 0.98189716\n",
            "Iteration 244, loss = 0.98083903\n",
            "Iteration 245, loss = 0.97977522\n",
            "Iteration 246, loss = 0.97870557\n",
            "Iteration 247, loss = 0.97763012\n",
            "Iteration 248, loss = 0.97654905\n",
            "Iteration 249, loss = 0.97546228\n",
            "Iteration 250, loss = 0.97436980\n",
            "Iteration 251, loss = 0.97327182\n",
            "Iteration 252, loss = 0.97216831\n",
            "Iteration 253, loss = 0.97105922\n",
            "Iteration 254, loss = 0.96994477\n",
            "Iteration 255, loss = 0.96882499\n",
            "Iteration 256, loss = 0.96769978\n",
            "Iteration 257, loss = 0.96656930\n",
            "Iteration 258, loss = 0.96543370\n",
            "Iteration 259, loss = 0.96429284\n",
            "Iteration 260, loss = 0.96314685\n",
            "Iteration 261, loss = 0.96199590\n",
            "Iteration 262, loss = 0.96083990\n",
            "Iteration 263, loss = 0.95967890\n",
            "Iteration 264, loss = 0.95851307\n",
            "Iteration 265, loss = 0.95734239\n",
            "Iteration 266, loss = 0.95616686\n",
            "Iteration 267, loss = 0.95498664\n",
            "Iteration 268, loss = 0.95380173\n",
            "Iteration 269, loss = 0.95261213\n",
            "Iteration 270, loss = 0.95141796\n",
            "Iteration 271, loss = 0.95021925\n",
            "Iteration 272, loss = 0.94901600\n",
            "Iteration 273, loss = 0.94780831\n",
            "Iteration 274, loss = 0.94659619\n",
            "Iteration 275, loss = 0.94537968\n",
            "Iteration 276, loss = 0.94415883\n",
            "Iteration 277, loss = 0.94293369\n",
            "Iteration 278, loss = 0.94170428\n",
            "Iteration 279, loss = 0.94047065\n",
            "Iteration 280, loss = 0.93923282\n",
            "Iteration 281, loss = 0.93799086\n",
            "Iteration 282, loss = 0.93674478\n",
            "Iteration 283, loss = 0.93549463\n",
            "Iteration 284, loss = 0.93424046\n",
            "Iteration 285, loss = 0.93298230\n",
            "Iteration 286, loss = 0.93172020\n",
            "Iteration 287, loss = 0.93045422\n",
            "Iteration 288, loss = 0.92918438\n",
            "Iteration 289, loss = 0.92791074\n",
            "Iteration 290, loss = 0.92663337\n",
            "Iteration 291, loss = 0.92535231\n",
            "Iteration 292, loss = 0.92406763\n",
            "Iteration 293, loss = 0.92277939\n",
            "Iteration 294, loss = 0.92148766\n",
            "Iteration 295, loss = 0.92019250\n",
            "Iteration 296, loss = 0.91889398\n",
            "Iteration 297, loss = 0.91759219\n",
            "Iteration 298, loss = 0.91628720\n",
            "Iteration 299, loss = 0.91497908\n",
            "Iteration 300, loss = 0.91366791\n",
            "Iteration 301, loss = 0.91235378\n",
            "Iteration 302, loss = 0.91103675\n",
            "Iteration 303, loss = 0.90971690\n",
            "Iteration 304, loss = 0.90839431\n",
            "Iteration 305, loss = 0.90706904\n",
            "Iteration 306, loss = 0.90574117\n",
            "Iteration 307, loss = 0.90441074\n",
            "Iteration 308, loss = 0.90307781\n",
            "Iteration 309, loss = 0.90174243\n",
            "Iteration 310, loss = 0.90040462\n",
            "Iteration 311, loss = 0.89906442\n",
            "Iteration 312, loss = 0.89772183\n",
            "Iteration 313, loss = 0.89637685\n",
            "Iteration 314, loss = 0.89502943\n",
            "Iteration 315, loss = 0.89367947\n",
            "Iteration 316, loss = 0.89232665\n",
            "Iteration 317, loss = 0.89096997\n",
            "Iteration 318, loss = 0.88960606\n",
            "Iteration 319, loss = 0.88822278\n",
            "Iteration 320, loss = 0.88677621\n",
            "Iteration 321, loss = 0.88512573\n",
            "Iteration 322, loss = 0.88332964\n",
            "Iteration 323, loss = 0.88162934\n",
            "Iteration 324, loss = 0.87993522\n",
            "Iteration 325, loss = 0.87790811\n",
            "Iteration 326, loss = 0.87574057\n",
            "Iteration 327, loss = 0.87342194\n",
            "Iteration 328, loss = 0.87136732\n",
            "Iteration 329, loss = 0.86835209\n",
            "Iteration 330, loss = 0.86561382\n",
            "Iteration 331, loss = 0.86244686\n",
            "Iteration 332, loss = 0.85984249\n",
            "Iteration 333, loss = 0.85657234\n",
            "Iteration 334, loss = 0.85398605\n",
            "Iteration 335, loss = 0.85088284\n",
            "Iteration 336, loss = 0.84854368\n",
            "Iteration 337, loss = 0.84568751\n",
            "Iteration 338, loss = 0.84333035\n",
            "Iteration 339, loss = 0.84088825\n",
            "Iteration 340, loss = 0.83849063\n",
            "Iteration 341, loss = 0.83632892\n",
            "Iteration 342, loss = 0.83405794\n",
            "Iteration 343, loss = 0.83188753\n",
            "Iteration 344, loss = 0.82983246\n",
            "Iteration 345, loss = 0.82769327\n",
            "Iteration 346, loss = 0.82566512\n",
            "Iteration 347, loss = 0.82362501\n",
            "Iteration 348, loss = 0.82159029\n",
            "Iteration 349, loss = 0.81956566\n",
            "Iteration 350, loss = 0.81755893\n",
            "Iteration 351, loss = 0.81552846\n",
            "Iteration 352, loss = 0.81352210\n",
            "Iteration 353, loss = 0.81145639\n",
            "Iteration 354, loss = 0.80937280\n",
            "Iteration 355, loss = 0.80731486\n",
            "Iteration 356, loss = 0.80519640\n",
            "Iteration 357, loss = 0.80307208\n",
            "Iteration 358, loss = 0.80097653\n",
            "Iteration 359, loss = 0.79884112\n",
            "Iteration 360, loss = 0.79670594\n",
            "Iteration 361, loss = 0.79460189\n",
            "Iteration 362, loss = 0.79245514\n",
            "Iteration 363, loss = 0.79034813\n",
            "Iteration 364, loss = 0.78824953\n",
            "Iteration 365, loss = 0.78612998\n",
            "Iteration 366, loss = 0.78405312\n",
            "Iteration 367, loss = 0.78197985\n",
            "Iteration 368, loss = 0.77991684\n",
            "Iteration 369, loss = 0.77786686\n",
            "Iteration 370, loss = 0.77584853\n",
            "Iteration 371, loss = 0.77382328\n",
            "Iteration 372, loss = 0.77183671\n",
            "Iteration 373, loss = 0.76985424\n",
            "Iteration 374, loss = 0.76789475\n",
            "Iteration 375, loss = 0.76595786\n",
            "Iteration 376, loss = 0.76403128\n",
            "Iteration 377, loss = 0.76213601\n",
            "Iteration 378, loss = 0.76024829\n",
            "Iteration 379, loss = 0.75839115\n",
            "Iteration 380, loss = 0.75654333\n",
            "Iteration 381, loss = 0.75472481\n",
            "Iteration 382, loss = 0.75291607\n",
            "Iteration 383, loss = 0.75113512\n",
            "Iteration 384, loss = 0.74936371\n",
            "Iteration 385, loss = 0.74761916\n",
            "Iteration 386, loss = 0.74588494\n",
            "Iteration 387, loss = 0.74417492\n",
            "Iteration 388, loss = 0.74247782\n",
            "Iteration 389, loss = 0.74079991\n",
            "Iteration 390, loss = 0.73913837\n",
            "Iteration 391, loss = 0.73749118\n",
            "Iteration 392, loss = 0.73586336\n",
            "Iteration 393, loss = 0.73424658\n",
            "Iteration 394, loss = 0.73264920\n",
            "Iteration 395, loss = 0.73106380\n",
            "Iteration 396, loss = 0.72949408\n",
            "Iteration 397, loss = 0.72793895\n",
            "Iteration 398, loss = 0.72639641\n",
            "Iteration 399, loss = 0.72486920\n",
            "Iteration 400, loss = 0.72335437\n",
            "Iteration 401, loss = 0.72185276\n",
            "Iteration 402, loss = 0.72036575\n",
            "Iteration 403, loss = 0.71889034\n",
            "Iteration 404, loss = 0.71743065\n",
            "Iteration 405, loss = 0.71598743\n",
            "Iteration 406, loss = 0.71456615\n",
            "Iteration 407, loss = 0.71317959\n",
            "Iteration 408, loss = 0.71184437\n",
            "Iteration 409, loss = 0.71053080\n",
            "Iteration 410, loss = 0.70915055\n",
            "Iteration 411, loss = 0.70762024\n",
            "Iteration 412, loss = 0.70616395\n",
            "Iteration 413, loss = 0.70489702\n",
            "Iteration 414, loss = 0.70361483\n",
            "Iteration 415, loss = 0.70220040\n",
            "Iteration 416, loss = 0.70081695\n",
            "Iteration 417, loss = 0.69957633\n",
            "Iteration 418, loss = 0.69830278\n",
            "Iteration 419, loss = 0.69693588\n",
            "Iteration 420, loss = 0.69564702\n",
            "Iteration 421, loss = 0.69443004\n",
            "Iteration 422, loss = 0.69314576\n",
            "Iteration 423, loss = 0.69184030\n",
            "Iteration 424, loss = 0.69061855\n",
            "Iteration 425, loss = 0.68938409\n",
            "Iteration 426, loss = 0.68810781\n",
            "Iteration 427, loss = 0.68681774\n",
            "Iteration 428, loss = 0.68567782\n",
            "Iteration 429, loss = 0.68432111\n",
            "Iteration 430, loss = 0.68306897\n",
            "Iteration 431, loss = 0.68178645\n",
            "Iteration 432, loss = 0.68041667\n",
            "Iteration 433, loss = 0.67915158\n",
            "Iteration 434, loss = 0.67774412\n",
            "Iteration 435, loss = 0.67635849\n",
            "Iteration 436, loss = 0.67506775\n",
            "Iteration 437, loss = 0.67367876\n",
            "Iteration 438, loss = 0.67232546\n",
            "Iteration 439, loss = 0.67102513\n",
            "Iteration 440, loss = 0.66968137\n",
            "Iteration 441, loss = 0.66838500\n",
            "Iteration 442, loss = 0.66708622\n",
            "Iteration 443, loss = 0.66578209\n",
            "Iteration 444, loss = 0.66452601\n",
            "Iteration 445, loss = 0.66327714\n",
            "Iteration 446, loss = 0.66200243\n",
            "Iteration 447, loss = 0.66075710\n",
            "Iteration 448, loss = 0.65954190\n",
            "Iteration 449, loss = 0.65831524\n",
            "Iteration 450, loss = 0.65708939\n",
            "Iteration 451, loss = 0.65588523\n",
            "Iteration 452, loss = 0.65468761\n",
            "Iteration 453, loss = 0.65349320\n",
            "Iteration 454, loss = 0.65231075\n",
            "Iteration 455, loss = 0.65113033\n",
            "Iteration 456, loss = 0.64995086\n",
            "Iteration 457, loss = 0.64878674\n",
            "Iteration 458, loss = 0.64763112\n",
            "Iteration 459, loss = 0.64647270\n",
            "Iteration 460, loss = 0.64532004\n",
            "Iteration 461, loss = 0.64417481\n",
            "Iteration 462, loss = 0.64303141\n",
            "Iteration 463, loss = 0.64189499\n",
            "Iteration 464, loss = 0.64076556\n",
            "Iteration 465, loss = 0.63963526\n",
            "Iteration 466, loss = 0.63850846\n",
            "Iteration 467, loss = 0.63738646\n",
            "Iteration 468, loss = 0.63626426\n",
            "Iteration 469, loss = 0.63514683\n",
            "Iteration 470, loss = 0.63403317\n",
            "Iteration 471, loss = 0.63291952\n",
            "Iteration 472, loss = 0.63180994\n",
            "Iteration 473, loss = 0.63070092\n",
            "Iteration 474, loss = 0.62959261\n",
            "Iteration 475, loss = 0.62848666\n",
            "Iteration 476, loss = 0.62737996\n",
            "Iteration 477, loss = 0.62627566\n",
            "Iteration 478, loss = 0.62517117\n",
            "Iteration 479, loss = 0.62406910\n",
            "Iteration 480, loss = 0.62296996\n",
            "Iteration 481, loss = 0.62187587\n",
            "Iteration 482, loss = 0.62079375\n",
            "Iteration 483, loss = 0.61972898\n",
            "Iteration 484, loss = 0.61869609\n",
            "Iteration 485, loss = 0.61766871\n",
            "Iteration 486, loss = 0.61661971\n",
            "Iteration 487, loss = 0.61543005\n",
            "Iteration 488, loss = 0.61417546\n",
            "Iteration 489, loss = 0.61298916\n",
            "Iteration 490, loss = 0.61194213\n",
            "Iteration 491, loss = 0.61093848\n",
            "Iteration 492, loss = 0.60983585\n",
            "Iteration 493, loss = 0.60864423\n",
            "Iteration 494, loss = 0.60746572\n",
            "Iteration 495, loss = 0.60638457\n",
            "Iteration 496, loss = 0.60534536\n",
            "Iteration 497, loss = 0.60422996\n",
            "Iteration 498, loss = 0.60298849\n",
            "Iteration 499, loss = 0.60178412\n",
            "Iteration 500, loss = 0.60055312\n",
            "Iteration 501, loss = 0.59939609\n",
            "Iteration 502, loss = 0.59820105\n",
            "Iteration 503, loss = 0.59692970\n",
            "Iteration 504, loss = 0.59569242\n",
            "Iteration 505, loss = 0.59434267\n",
            "Iteration 506, loss = 0.59307060\n",
            "Iteration 507, loss = 0.59176310\n",
            "Iteration 508, loss = 0.59046845\n",
            "Iteration 509, loss = 0.58920069\n",
            "Iteration 510, loss = 0.58788222\n",
            "Iteration 511, loss = 0.58658478\n",
            "Iteration 512, loss = 0.58527853\n",
            "Iteration 513, loss = 0.58394718\n",
            "Iteration 514, loss = 0.58262998\n",
            "Iteration 515, loss = 0.58131388\n",
            "Iteration 516, loss = 0.57998337\n",
            "Iteration 517, loss = 0.57866246\n",
            "Iteration 518, loss = 0.57734795\n",
            "Iteration 519, loss = 0.57602116\n",
            "Iteration 520, loss = 0.57470427\n",
            "Iteration 521, loss = 0.57339957\n",
            "Iteration 522, loss = 0.57210632\n",
            "Iteration 523, loss = 0.57087271\n",
            "Iteration 524, loss = 0.56973665\n",
            "Iteration 525, loss = 0.56867131\n",
            "Iteration 526, loss = 0.56739158\n",
            "Iteration 527, loss = 0.56572225\n",
            "Iteration 528, loss = 0.56413553\n",
            "Iteration 529, loss = 0.56304125\n",
            "Iteration 530, loss = 0.56193307\n",
            "Iteration 531, loss = 0.56038276\n",
            "Iteration 532, loss = 0.55889080\n",
            "Iteration 533, loss = 0.55776762\n",
            "Iteration 534, loss = 0.55650578\n",
            "Iteration 535, loss = 0.55498750\n",
            "Iteration 536, loss = 0.55365747\n",
            "Iteration 537, loss = 0.55248082\n",
            "Iteration 538, loss = 0.55108822\n",
            "Iteration 539, loss = 0.54964919\n",
            "Iteration 540, loss = 0.54840630\n",
            "Iteration 541, loss = 0.54711685\n",
            "Iteration 542, loss = 0.54567863\n",
            "Iteration 543, loss = 0.54433913\n",
            "Iteration 544, loss = 0.54307307\n",
            "Iteration 545, loss = 0.54168943\n",
            "Iteration 546, loss = 0.54028993\n",
            "Iteration 547, loss = 0.53898272\n",
            "Iteration 548, loss = 0.53765048\n",
            "Iteration 549, loss = 0.53624233\n",
            "Iteration 550, loss = 0.53487366\n",
            "Iteration 551, loss = 0.53354527\n",
            "Iteration 552, loss = 0.53216661\n",
            "Iteration 553, loss = 0.53075876\n",
            "Iteration 554, loss = 0.52938929\n",
            "Iteration 555, loss = 0.52802765\n",
            "Iteration 556, loss = 0.52662551\n",
            "Iteration 557, loss = 0.52521063\n",
            "Iteration 558, loss = 0.52382025\n",
            "Iteration 559, loss = 0.52243054\n",
            "Iteration 560, loss = 0.52101241\n",
            "Iteration 561, loss = 0.51958015\n",
            "Iteration 562, loss = 0.51815821\n",
            "Iteration 563, loss = 0.51674088\n",
            "Iteration 564, loss = 0.51531016\n",
            "Iteration 565, loss = 0.51386183\n",
            "Iteration 566, loss = 0.51240485\n",
            "Iteration 567, loss = 0.51094983\n",
            "Iteration 568, loss = 0.50949663\n",
            "Iteration 569, loss = 0.50804250\n",
            "Iteration 570, loss = 0.50658880\n",
            "Iteration 571, loss = 0.50513656\n",
            "Iteration 572, loss = 0.50369234\n",
            "Iteration 573, loss = 0.50223123\n",
            "Iteration 574, loss = 0.50075292\n",
            "Iteration 575, loss = 0.49924740\n",
            "Iteration 576, loss = 0.49774310\n",
            "Iteration 577, loss = 0.49625837\n",
            "Iteration 578, loss = 0.49479137\n",
            "Iteration 579, loss = 0.49333905\n",
            "Iteration 580, loss = 0.49190354\n",
            "Iteration 581, loss = 0.49048470\n",
            "Iteration 582, loss = 0.48901449\n",
            "Iteration 583, loss = 0.48749871\n",
            "Iteration 584, loss = 0.48595174\n",
            "Iteration 585, loss = 0.48444746\n",
            "Iteration 586, loss = 0.48300320\n",
            "Iteration 587, loss = 0.48156215\n",
            "Iteration 588, loss = 0.48008761\n",
            "Iteration 589, loss = 0.47856235\n",
            "Iteration 590, loss = 0.47704166\n",
            "Iteration 591, loss = 0.47556439\n",
            "Iteration 592, loss = 0.47411015\n",
            "Iteration 593, loss = 0.47263721\n",
            "Iteration 594, loss = 0.47113427\n",
            "Iteration 595, loss = 0.46962427\n",
            "Iteration 596, loss = 0.46813983\n",
            "Iteration 597, loss = 0.46667566\n",
            "Iteration 598, loss = 0.46520255\n",
            "Iteration 599, loss = 0.46371258\n",
            "Iteration 600, loss = 0.46221506\n",
            "Iteration 601, loss = 0.46073055\n",
            "Iteration 602, loss = 0.45926141\n",
            "Iteration 603, loss = 0.45779189\n",
            "Iteration 604, loss = 0.45631204\n",
            "Iteration 605, loss = 0.45482644\n",
            "Iteration 606, loss = 0.45334559\n",
            "Iteration 607, loss = 0.45187515\n",
            "Iteration 608, loss = 0.45040948\n",
            "Iteration 609, loss = 0.44893999\n",
            "Iteration 610, loss = 0.44746715\n",
            "Iteration 611, loss = 0.44599341\n",
            "Iteration 612, loss = 0.44452437\n",
            "Iteration 613, loss = 0.44306187\n",
            "Iteration 614, loss = 0.44160196\n",
            "Iteration 615, loss = 0.44014212\n",
            "Iteration 616, loss = 0.43868108\n",
            "Iteration 617, loss = 0.43722021\n",
            "Iteration 618, loss = 0.43576195\n",
            "Iteration 619, loss = 0.43430772\n",
            "Iteration 620, loss = 0.43285694\n",
            "Iteration 621, loss = 0.43140890\n",
            "Iteration 622, loss = 0.42996272\n",
            "Iteration 623, loss = 0.42851739\n",
            "Iteration 624, loss = 0.42707356\n",
            "Iteration 625, loss = 0.42563147\n",
            "Iteration 626, loss = 0.42419164\n",
            "Iteration 627, loss = 0.42275469\n",
            "Iteration 628, loss = 0.42132069\n",
            "Iteration 629, loss = 0.41988957\n",
            "Iteration 630, loss = 0.41846136\n",
            "Iteration 631, loss = 0.41703599\n",
            "Iteration 632, loss = 0.41561329\n",
            "Iteration 633, loss = 0.41419343\n",
            "Iteration 634, loss = 0.41277642\n",
            "Iteration 635, loss = 0.41136235\n",
            "Iteration 636, loss = 0.40995141\n",
            "Iteration 637, loss = 0.40854391\n",
            "Iteration 638, loss = 0.40713983\n",
            "Iteration 639, loss = 0.40573979\n",
            "Iteration 640, loss = 0.40434360\n",
            "Iteration 641, loss = 0.40295200\n",
            "Iteration 642, loss = 0.40156377\n",
            "Iteration 643, loss = 0.40017937\n",
            "Iteration 644, loss = 0.39879519\n",
            "Iteration 645, loss = 0.39741140\n",
            "Iteration 646, loss = 0.39602474\n",
            "Iteration 647, loss = 0.39463814\n",
            "Iteration 648, loss = 0.39325367\n",
            "Iteration 649, loss = 0.39187567\n",
            "Iteration 650, loss = 0.39050600\n",
            "Iteration 651, loss = 0.38914452\n",
            "Iteration 652, loss = 0.38778965\n",
            "Iteration 653, loss = 0.38643924\n",
            "Iteration 654, loss = 0.38509178\n",
            "Iteration 655, loss = 0.38374570\n",
            "Iteration 656, loss = 0.38240108\n",
            "Iteration 657, loss = 0.38105768\n",
            "Iteration 658, loss = 0.37971700\n",
            "Iteration 659, loss = 0.37837994\n",
            "Iteration 660, loss = 0.37704783\n",
            "Iteration 661, loss = 0.37572118\n",
            "Iteration 662, loss = 0.37440010\n",
            "Iteration 663, loss = 0.37308431\n",
            "Iteration 664, loss = 0.37177345\n",
            "Iteration 665, loss = 0.37046720\n",
            "Iteration 666, loss = 0.36916536\n",
            "Iteration 667, loss = 0.36786795\n",
            "Iteration 668, loss = 0.36657482\n",
            "Iteration 669, loss = 0.36528627\n",
            "Iteration 670, loss = 0.36400198\n",
            "Iteration 671, loss = 0.36272237\n",
            "Iteration 672, loss = 0.36144676\n",
            "Iteration 673, loss = 0.36017561\n",
            "Iteration 674, loss = 0.35890778\n",
            "Iteration 675, loss = 0.35764377\n",
            "Iteration 676, loss = 0.35638244\n",
            "Iteration 677, loss = 0.35512462\n",
            "Iteration 678, loss = 0.35386989\n",
            "Iteration 679, loss = 0.35261945\n",
            "Iteration 680, loss = 0.35137366\n",
            "Iteration 681, loss = 0.35013338\n",
            "Iteration 682, loss = 0.34889884\n",
            "Iteration 683, loss = 0.34767014\n",
            "Iteration 684, loss = 0.34644702\n",
            "Iteration 685, loss = 0.34522876\n",
            "Iteration 686, loss = 0.34401293\n",
            "Iteration 687, loss = 0.34279095\n",
            "Iteration 688, loss = 0.34154526\n",
            "Iteration 689, loss = 0.34041495\n",
            "Iteration 690, loss = 0.33912700\n",
            "Iteration 691, loss = 0.33796490\n",
            "Iteration 692, loss = 0.33681170\n",
            "Iteration 693, loss = 0.33562308\n",
            "Iteration 694, loss = 0.33435141\n",
            "Iteration 695, loss = 0.33303366\n",
            "Iteration 696, loss = 0.33181998\n",
            "Iteration 697, loss = 0.33045081\n",
            "Iteration 698, loss = 0.32923246\n",
            "Iteration 699, loss = 0.32805469\n",
            "Iteration 700, loss = 0.32676855\n",
            "Iteration 701, loss = 0.32548849\n",
            "Iteration 702, loss = 0.32422176\n",
            "Iteration 703, loss = 0.32289386\n",
            "Iteration 704, loss = 0.32169258\n",
            "Iteration 705, loss = 0.32044160\n",
            "Iteration 706, loss = 0.31916523\n",
            "Iteration 707, loss = 0.31795179\n",
            "Iteration 708, loss = 0.31665097\n",
            "Iteration 709, loss = 0.31542286\n",
            "Iteration 710, loss = 0.31419527\n",
            "Iteration 711, loss = 0.31294471\n",
            "Iteration 712, loss = 0.31175408\n",
            "Iteration 713, loss = 0.31050814\n",
            "Iteration 714, loss = 0.30929380\n",
            "Iteration 715, loss = 0.30808334\n",
            "Iteration 716, loss = 0.30686221\n",
            "Iteration 717, loss = 0.30568649\n",
            "Iteration 718, loss = 0.30448582\n",
            "Iteration 719, loss = 0.30330975\n",
            "Iteration 720, loss = 0.30212716\n",
            "Iteration 721, loss = 0.30094280\n",
            "Iteration 722, loss = 0.29978047\n",
            "Iteration 723, loss = 0.29860982\n",
            "Iteration 724, loss = 0.29746624\n",
            "Iteration 725, loss = 0.29631548\n",
            "Iteration 726, loss = 0.29517789\n",
            "Iteration 727, loss = 0.29404069\n",
            "Iteration 728, loss = 0.29290562\n",
            "Iteration 729, loss = 0.29178111\n",
            "Iteration 730, loss = 0.29065651\n",
            "Iteration 731, loss = 0.28954696\n",
            "Iteration 732, loss = 0.28843715\n",
            "Iteration 733, loss = 0.28734045\n",
            "Iteration 734, loss = 0.28624291\n",
            "Iteration 735, loss = 0.28515565\n",
            "Iteration 736, loss = 0.28406847\n",
            "Iteration 737, loss = 0.28298898\n",
            "Iteration 738, loss = 0.28191093\n",
            "Iteration 739, loss = 0.28084005\n",
            "Iteration 740, loss = 0.27977162\n",
            "Iteration 741, loss = 0.27870968\n",
            "Iteration 742, loss = 0.27765021\n",
            "Iteration 743, loss = 0.27659745\n",
            "Iteration 744, loss = 0.27554786\n",
            "Iteration 745, loss = 0.27450471\n",
            "Iteration 746, loss = 0.27346647\n",
            "Iteration 747, loss = 0.27243814\n",
            "Iteration 748, loss = 0.27142628\n",
            "Iteration 749, loss = 0.27043703\n",
            "Iteration 750, loss = 0.26949317\n",
            "Iteration 751, loss = 0.26857492\n",
            "Iteration 752, loss = 0.26765541\n",
            "Iteration 753, loss = 0.26654645\n",
            "Iteration 754, loss = 0.26534237\n",
            "Iteration 755, loss = 0.26424785\n",
            "Iteration 756, loss = 0.26333021\n",
            "Iteration 757, loss = 0.26245068\n",
            "Iteration 758, loss = 0.26141174\n",
            "Iteration 759, loss = 0.26030218\n",
            "Iteration 760, loss = 0.25927795\n",
            "Iteration 761, loss = 0.25836928\n",
            "Iteration 762, loss = 0.25744711\n",
            "Iteration 763, loss = 0.25641722\n",
            "Iteration 764, loss = 0.25537356\n",
            "Iteration 765, loss = 0.25440599\n",
            "Iteration 766, loss = 0.25349741\n",
            "Iteration 767, loss = 0.25256428\n",
            "Iteration 768, loss = 0.25157306\n",
            "Iteration 769, loss = 0.25057761\n",
            "Iteration 770, loss = 0.24962780\n",
            "Iteration 771, loss = 0.24871945\n",
            "Iteration 772, loss = 0.24781766\n",
            "Iteration 773, loss = 0.24688992\n",
            "Iteration 774, loss = 0.24594643\n",
            "Iteration 775, loss = 0.24500309\n",
            "Iteration 776, loss = 0.24408399\n",
            "Iteration 777, loss = 0.24319293\n",
            "Iteration 778, loss = 0.24232020\n",
            "Iteration 779, loss = 0.24146028\n",
            "Iteration 780, loss = 0.24059860\n",
            "Iteration 781, loss = 0.23973969\n",
            "Iteration 782, loss = 0.23886340\n",
            "Iteration 783, loss = 0.23798306\n",
            "Iteration 784, loss = 0.23709697\n",
            "Iteration 785, loss = 0.23622576\n",
            "Iteration 786, loss = 0.23537493\n",
            "Iteration 787, loss = 0.23454331\n",
            "Iteration 788, loss = 0.23372531\n",
            "Iteration 789, loss = 0.23291263\n",
            "Iteration 790, loss = 0.23210324\n",
            "Iteration 791, loss = 0.23128579\n",
            "Iteration 792, loss = 0.23046401\n",
            "Iteration 793, loss = 0.22963403\n",
            "Iteration 794, loss = 0.22880976\n",
            "Iteration 795, loss = 0.22799782\n",
            "Iteration 796, loss = 0.22720180\n",
            "Iteration 797, loss = 0.22641862\n",
            "Iteration 798, loss = 0.22564206\n",
            "Iteration 799, loss = 0.22486762\n",
            "Iteration 800, loss = 0.22409019\n",
            "Iteration 801, loss = 0.22331185\n",
            "Iteration 802, loss = 0.22253348\n",
            "Iteration 803, loss = 0.22176088\n",
            "Iteration 804, loss = 0.22099659\n",
            "Iteration 805, loss = 0.22024133\n",
            "Iteration 806, loss = 0.21949364\n",
            "Iteration 807, loss = 0.21875121\n",
            "Iteration 808, loss = 0.21801248\n",
            "Iteration 809, loss = 0.21727561\n",
            "Iteration 810, loss = 0.21654107\n",
            "Iteration 811, loss = 0.21580796\n",
            "Iteration 812, loss = 0.21507786\n",
            "Iteration 813, loss = 0.21435078\n",
            "Iteration 814, loss = 0.21362781\n",
            "Iteration 815, loss = 0.21290895\n",
            "Iteration 816, loss = 0.21219437\n",
            "Iteration 817, loss = 0.21148389\n",
            "Iteration 818, loss = 0.21077724\n",
            "Iteration 819, loss = 0.21007418\n",
            "Iteration 820, loss = 0.20937449\n",
            "Iteration 821, loss = 0.20867801\n",
            "Iteration 822, loss = 0.20798467\n",
            "Iteration 823, loss = 0.20729452\n",
            "Iteration 824, loss = 0.20660781\n",
            "Iteration 825, loss = 0.20592530\n",
            "Iteration 826, loss = 0.20524859\n",
            "Iteration 827, loss = 0.20458212\n",
            "Iteration 828, loss = 0.20393289\n",
            "Iteration 829, loss = 0.20332038\n",
            "Iteration 830, loss = 0.20274767\n",
            "Iteration 831, loss = 0.20222730\n",
            "Iteration 832, loss = 0.20158545\n",
            "Iteration 833, loss = 0.20078633\n",
            "Iteration 834, loss = 0.19988527\n",
            "Iteration 835, loss = 0.19917125\n",
            "Iteration 836, loss = 0.19864124\n",
            "Iteration 837, loss = 0.19805031\n",
            "Iteration 838, loss = 0.19728538\n",
            "Iteration 839, loss = 0.19647355\n",
            "Iteration 840, loss = 0.19581584\n",
            "Iteration 841, loss = 0.19524860\n",
            "Iteration 842, loss = 0.19458177\n",
            "Iteration 843, loss = 0.19381155\n",
            "Iteration 844, loss = 0.19307804\n",
            "Iteration 845, loss = 0.19245018\n",
            "Iteration 846, loss = 0.19182575\n",
            "Iteration 847, loss = 0.19111199\n",
            "Iteration 848, loss = 0.19037772\n",
            "Iteration 849, loss = 0.18970958\n",
            "Iteration 850, loss = 0.18907957\n",
            "Iteration 851, loss = 0.18841118\n",
            "Iteration 852, loss = 0.18770056\n",
            "Iteration 853, loss = 0.18701328\n",
            "Iteration 854, loss = 0.18636935\n",
            "Iteration 855, loss = 0.18572452\n",
            "Iteration 856, loss = 0.18504876\n",
            "Iteration 857, loss = 0.18436531\n",
            "Iteration 858, loss = 0.18370972\n",
            "Iteration 859, loss = 0.18307447\n",
            "Iteration 860, loss = 0.18242879\n",
            "Iteration 861, loss = 0.18176697\n",
            "Iteration 862, loss = 0.18111016\n",
            "Iteration 863, loss = 0.18047371\n",
            "Iteration 864, loss = 0.17984685\n",
            "Iteration 865, loss = 0.17921254\n",
            "Iteration 866, loss = 0.17857187\n",
            "Iteration 867, loss = 0.17793759\n",
            "Iteration 868, loss = 0.17731609\n",
            "Iteration 869, loss = 0.17670146\n",
            "Iteration 870, loss = 0.17608519\n",
            "Iteration 871, loss = 0.17546640\n",
            "Iteration 872, loss = 0.17485092\n",
            "Iteration 873, loss = 0.17424395\n",
            "Iteration 874, loss = 0.17364433\n",
            "Iteration 875, loss = 0.17304695\n",
            "Iteration 876, loss = 0.17244923\n",
            "Iteration 877, loss = 0.17185300\n",
            "Iteration 878, loss = 0.17126161\n",
            "Iteration 879, loss = 0.17067627\n",
            "Iteration 880, loss = 0.17009587\n",
            "Iteration 881, loss = 0.16951834\n",
            "Iteration 882, loss = 0.16894237\n",
            "Iteration 883, loss = 0.16836855\n",
            "Iteration 884, loss = 0.16779834\n",
            "Iteration 885, loss = 0.16723262\n",
            "Iteration 886, loss = 0.16667129\n",
            "Iteration 887, loss = 0.16611366\n",
            "Iteration 888, loss = 0.16555901\n",
            "Iteration 889, loss = 0.16500688\n",
            "Iteration 890, loss = 0.16445740\n",
            "Iteration 891, loss = 0.16391099\n",
            "Iteration 892, loss = 0.16336799\n",
            "Iteration 893, loss = 0.16282852\n",
            "Iteration 894, loss = 0.16229256\n",
            "Iteration 895, loss = 0.16175997\n",
            "Iteration 896, loss = 0.16123050\n",
            "Iteration 897, loss = 0.16070401\n",
            "Iteration 898, loss = 0.16018043\n",
            "Iteration 899, loss = 0.15965972\n",
            "Iteration 900, loss = 0.15914186\n",
            "Iteration 901, loss = 0.15862691\n",
            "Iteration 902, loss = 0.15811485\n",
            "Iteration 903, loss = 0.15760570\n",
            "Iteration 904, loss = 0.15709944\n",
            "Iteration 905, loss = 0.15659612\n",
            "Iteration 906, loss = 0.15609572\n",
            "Iteration 907, loss = 0.15559830\n",
            "Iteration 908, loss = 0.15510398\n",
            "Iteration 909, loss = 0.15461299\n",
            "Iteration 910, loss = 0.15412572\n",
            "Iteration 911, loss = 0.15364310\n",
            "Iteration 912, loss = 0.15316654\n",
            "Iteration 913, loss = 0.15269930\n",
            "Iteration 914, loss = 0.15224575\n",
            "Iteration 915, loss = 0.15181555\n",
            "Iteration 916, loss = 0.15141338\n",
            "Iteration 917, loss = 0.15104525\n",
            "Iteration 918, loss = 0.15065611\n",
            "Iteration 919, loss = 0.15019008\n",
            "Iteration 920, loss = 0.14957870\n",
            "Iteration 921, loss = 0.14895384\n",
            "Iteration 922, loss = 0.14846176\n",
            "Iteration 923, loss = 0.14810274\n",
            "Iteration 924, loss = 0.14773911\n",
            "Iteration 925, loss = 0.14725221\n",
            "Iteration 926, loss = 0.14669781\n",
            "Iteration 927, loss = 0.14620971\n",
            "Iteration 928, loss = 0.14582361\n",
            "Iteration 929, loss = 0.14544135\n",
            "Iteration 930, loss = 0.14497755\n",
            "Iteration 931, loss = 0.14447514\n",
            "Iteration 932, loss = 0.14402310\n",
            "Iteration 933, loss = 0.14363181\n",
            "Iteration 934, loss = 0.14323289\n",
            "Iteration 935, loss = 0.14278271\n",
            "Iteration 936, loss = 0.14231795\n",
            "Iteration 937, loss = 0.14188995\n",
            "Iteration 938, loss = 0.14149572\n",
            "Iteration 939, loss = 0.14109255\n",
            "Iteration 940, loss = 0.14065964\n",
            "Iteration 941, loss = 0.14022113\n",
            "Iteration 942, loss = 0.13980560\n",
            "Iteration 943, loss = 0.13941058\n",
            "Iteration 944, loss = 0.13901219\n",
            "Iteration 945, loss = 0.13859740\n",
            "Iteration 946, loss = 0.13817736\n",
            "Iteration 947, loss = 0.13776899\n",
            "Iteration 948, loss = 0.13737500\n",
            "Iteration 949, loss = 0.13698416\n",
            "Iteration 950, loss = 0.13658625\n",
            "Iteration 951, loss = 0.13618238\n",
            "Iteration 952, loss = 0.13578112\n",
            "Iteration 953, loss = 0.13538854\n",
            "Iteration 954, loss = 0.13500291\n",
            "Iteration 955, loss = 0.13461826\n",
            "Iteration 956, loss = 0.13423066\n",
            "Iteration 957, loss = 0.13384093\n",
            "Iteration 958, loss = 0.13345287\n",
            "Iteration 959, loss = 0.13306943\n",
            "Iteration 960, loss = 0.13269072\n",
            "Iteration 961, loss = 0.13231468\n",
            "Iteration 962, loss = 0.13193908\n",
            "Iteration 963, loss = 0.13156303\n",
            "Iteration 964, loss = 0.13118705\n",
            "Iteration 965, loss = 0.13081245\n",
            "Iteration 966, loss = 0.13044031\n",
            "Iteration 967, loss = 0.13007098\n",
            "Iteration 968, loss = 0.12970418\n",
            "Iteration 969, loss = 0.12933930\n",
            "Iteration 970, loss = 0.12897580\n",
            "Iteration 971, loss = 0.12861335\n",
            "Iteration 972, loss = 0.12825189\n",
            "Iteration 973, loss = 0.12789152\n",
            "Iteration 974, loss = 0.12753243\n",
            "Iteration 975, loss = 0.12717477\n",
            "Iteration 976, loss = 0.12681864\n",
            "Iteration 977, loss = 0.12646409\n",
            "Iteration 978, loss = 0.12611112\n",
            "Iteration 979, loss = 0.12575972\n",
            "Iteration 980, loss = 0.12540984\n",
            "Iteration 981, loss = 0.12506146\n",
            "Iteration 982, loss = 0.12471455\n",
            "Iteration 983, loss = 0.12436910\n",
            "Iteration 984, loss = 0.12402511\n",
            "Iteration 985, loss = 0.12368258\n",
            "Iteration 986, loss = 0.12334158\n",
            "Iteration 987, loss = 0.12300220\n",
            "Iteration 988, loss = 0.12266468\n",
            "Iteration 989, loss = 0.12232951\n",
            "Iteration 990, loss = 0.12199776\n",
            "Iteration 991, loss = 0.12167178\n",
            "Iteration 992, loss = 0.12135663\n",
            "Iteration 993, loss = 0.12106348\n",
            "Iteration 994, loss = 0.12081351\n",
            "Iteration 995, loss = 0.12064426\n",
            "Iteration 996, loss = 0.12057073\n",
            "Iteration 997, loss = 0.12050891\n",
            "Iteration 998, loss = 0.12012439\n",
            "Iteration 999, loss = 0.11935866\n",
            "Iteration 1000, loss = 0.11872302\n",
            "Iteration 1001, loss = 0.11860015\n",
            "Iteration 1002, loss = 0.11855925\n",
            "Iteration 1003, loss = 0.11808865\n",
            "Iteration 1004, loss = 0.11748491\n",
            "Iteration 1005, loss = 0.11724488\n",
            "Iteration 1006, loss = 0.11713387\n",
            "Iteration 1007, loss = 0.11672874\n",
            "Iteration 1008, loss = 0.11623401\n",
            "Iteration 1009, loss = 0.11600578\n",
            "Iteration 1010, loss = 0.11582176\n",
            "Iteration 1011, loss = 0.11541303\n",
            "Iteration 1012, loss = 0.11501251\n",
            "Iteration 1013, loss = 0.11479626\n",
            "Iteration 1014, loss = 0.11454457\n",
            "Iteration 1015, loss = 0.11415554\n",
            "Iteration 1016, loss = 0.11382217\n",
            "Iteration 1017, loss = 0.11359609\n",
            "Iteration 1018, loss = 0.11330932\n",
            "Iteration 1019, loss = 0.11294749\n",
            "Iteration 1020, loss = 0.11265252\n",
            "Iteration 1021, loss = 0.11240981\n",
            "Iteration 1022, loss = 0.11210739\n",
            "Iteration 1023, loss = 0.11177481\n",
            "Iteration 1024, loss = 0.11149633\n",
            "Iteration 1025, loss = 0.11123915\n",
            "Iteration 1026, loss = 0.11093919\n",
            "Iteration 1027, loss = 0.11062740\n",
            "Iteration 1028, loss = 0.11035414\n",
            "Iteration 1029, loss = 0.11009049\n",
            "Iteration 1030, loss = 0.10979730\n",
            "Iteration 1031, loss = 0.10949952\n",
            "Iteration 1032, loss = 0.10922740\n",
            "Iteration 1033, loss = 0.10896166\n",
            "Iteration 1034, loss = 0.10867777\n",
            "Iteration 1035, loss = 0.10838941\n",
            "Iteration 1036, loss = 0.10811746\n",
            "Iteration 1037, loss = 0.10785258\n",
            "Iteration 1038, loss = 0.10757711\n",
            "Iteration 1039, loss = 0.10729641\n",
            "Iteration 1040, loss = 0.10702493\n",
            "Iteration 1041, loss = 0.10676119\n",
            "Iteration 1042, loss = 0.10649345\n",
            "Iteration 1043, loss = 0.10622008\n",
            "Iteration 1044, loss = 0.10595015\n",
            "Iteration 1045, loss = 0.10568720\n",
            "Iteration 1046, loss = 0.10542516\n",
            "Iteration 1047, loss = 0.10515941\n",
            "Iteration 1048, loss = 0.10489295\n",
            "Iteration 1049, loss = 0.10463066\n",
            "Iteration 1050, loss = 0.10437198\n",
            "Iteration 1051, loss = 0.10411288\n",
            "Iteration 1052, loss = 0.10385206\n",
            "Iteration 1053, loss = 0.10359194\n",
            "Iteration 1054, loss = 0.10333483\n",
            "Iteration 1055, loss = 0.10307996\n",
            "Iteration 1056, loss = 0.10282516\n",
            "Iteration 1057, loss = 0.10256977\n",
            "Iteration 1058, loss = 0.10231502\n",
            "Iteration 1059, loss = 0.10206226\n",
            "Iteration 1060, loss = 0.10181134\n",
            "Iteration 1061, loss = 0.10156118\n",
            "Iteration 1062, loss = 0.10131109\n",
            "Iteration 1063, loss = 0.10106139\n",
            "Iteration 1064, loss = 0.10081285\n",
            "Iteration 1065, loss = 0.10056581\n",
            "Iteration 1066, loss = 0.10032000\n",
            "Iteration 1067, loss = 0.10007491\n",
            "Iteration 1068, loss = 0.09983028\n",
            "Iteration 1069, loss = 0.09958627\n",
            "Iteration 1070, loss = 0.09934322\n",
            "Iteration 1071, loss = 0.09910132\n",
            "Iteration 1072, loss = 0.09886051\n",
            "Iteration 1073, loss = 0.09862060\n",
            "Iteration 1074, loss = 0.09838141\n",
            "Iteration 1075, loss = 0.09814290\n",
            "Iteration 1076, loss = 0.09790515\n",
            "Iteration 1077, loss = 0.09766826\n",
            "Iteration 1078, loss = 0.09743231\n",
            "Iteration 1079, loss = 0.09719731\n",
            "Iteration 1080, loss = 0.09696321\n",
            "Iteration 1081, loss = 0.09672995\n",
            "Iteration 1082, loss = 0.09649748\n",
            "Iteration 1083, loss = 0.09626578\n",
            "Iteration 1084, loss = 0.09603485\n",
            "Iteration 1085, loss = 0.09580471\n",
            "Iteration 1086, loss = 0.09557539\n",
            "Iteration 1087, loss = 0.09534688\n",
            "Iteration 1088, loss = 0.09511920\n",
            "Iteration 1089, loss = 0.09489234\n",
            "Iteration 1090, loss = 0.09466629\n",
            "Iteration 1091, loss = 0.09444105\n",
            "Iteration 1092, loss = 0.09421661\n",
            "Iteration 1093, loss = 0.09399297\n",
            "Iteration 1094, loss = 0.09377011\n",
            "Iteration 1095, loss = 0.09354805\n",
            "Iteration 1096, loss = 0.09332678\n",
            "Iteration 1097, loss = 0.09310630\n",
            "Iteration 1098, loss = 0.09288664\n",
            "Iteration 1099, loss = 0.09266782\n",
            "Iteration 1100, loss = 0.09244990\n",
            "Iteration 1101, loss = 0.09223297\n",
            "Iteration 1102, loss = 0.09201725\n",
            "Iteration 1103, loss = 0.09180311\n",
            "Iteration 1104, loss = 0.09159139\n",
            "Iteration 1105, loss = 0.09138348\n",
            "Iteration 1106, loss = 0.09118295\n",
            "Iteration 1107, loss = 0.09099496\n",
            "Iteration 1108, loss = 0.09083475\n",
            "Iteration 1109, loss = 0.09071523\n",
            "Iteration 1110, loss = 0.09068684\n",
            "Iteration 1111, loss = 0.09069662\n",
            "Iteration 1112, loss = 0.09076560\n",
            "Iteration 1113, loss = 0.09043118\n",
            "Iteration 1114, loss = 0.08986868\n",
            "Iteration 1115, loss = 0.08930156\n",
            "Iteration 1116, loss = 0.08917315\n",
            "Iteration 1117, loss = 0.08927468\n",
            "Iteration 1118, loss = 0.08907351\n",
            "Iteration 1119, loss = 0.08862128\n",
            "Iteration 1120, loss = 0.08826665\n",
            "Iteration 1121, loss = 0.08821682\n",
            "Iteration 1122, loss = 0.08816305\n",
            "Iteration 1123, loss = 0.08782088\n",
            "Iteration 1124, loss = 0.08747643\n",
            "Iteration 1125, loss = 0.08734391\n",
            "Iteration 1126, loss = 0.08725215\n",
            "Iteration 1127, loss = 0.08700926\n",
            "Iteration 1128, loss = 0.08670021\n",
            "Iteration 1129, loss = 0.08652846\n",
            "Iteration 1130, loss = 0.08641913\n",
            "Iteration 1131, loss = 0.08619808\n",
            "Iteration 1132, loss = 0.08592971\n",
            "Iteration 1133, loss = 0.08574075\n",
            "Iteration 1134, loss = 0.08560403\n",
            "Iteration 1135, loss = 0.08541346\n",
            "Iteration 1136, loss = 0.08517037\n",
            "Iteration 1137, loss = 0.08497252\n",
            "Iteration 1138, loss = 0.08482107\n",
            "Iteration 1139, loss = 0.08463943\n",
            "Iteration 1140, loss = 0.08442077\n",
            "Iteration 1141, loss = 0.08421882\n",
            "Iteration 1142, loss = 0.08405317\n",
            "Iteration 1143, loss = 0.08388216\n",
            "Iteration 1144, loss = 0.08368018\n",
            "Iteration 1145, loss = 0.08347924\n",
            "Iteration 1146, loss = 0.08330308\n",
            "Iteration 1147, loss = 0.08313365\n",
            "Iteration 1148, loss = 0.08294789\n",
            "Iteration 1149, loss = 0.08275211\n",
            "Iteration 1150, loss = 0.08256843\n",
            "Iteration 1151, loss = 0.08239707\n",
            "Iteration 1152, loss = 0.08222094\n",
            "Iteration 1153, loss = 0.08203471\n",
            "Iteration 1154, loss = 0.08184888\n",
            "Iteration 1155, loss = 0.08167258\n",
            "Iteration 1156, loss = 0.08150063\n",
            "Iteration 1157, loss = 0.08132357\n",
            "Iteration 1158, loss = 0.08114184\n",
            "Iteration 1159, loss = 0.08096246\n",
            "Iteration 1160, loss = 0.08078890\n",
            "Iteration 1161, loss = 0.08061721\n",
            "Iteration 1162, loss = 0.08044246\n",
            "Iteration 1163, loss = 0.08026542\n",
            "Iteration 1164, loss = 0.08008998\n",
            "Iteration 1165, loss = 0.07991809\n",
            "Iteration 1166, loss = 0.07974778\n",
            "Iteration 1167, loss = 0.07957624\n",
            "Iteration 1168, loss = 0.07940334\n",
            "Iteration 1169, loss = 0.07923095\n",
            "Iteration 1170, loss = 0.07906066\n",
            "Iteration 1171, loss = 0.07889203\n",
            "Iteration 1172, loss = 0.07872356\n",
            "Iteration 1173, loss = 0.07855446\n",
            "Iteration 1174, loss = 0.07838518\n",
            "Iteration 1175, loss = 0.07821679\n",
            "Iteration 1176, loss = 0.07804976\n",
            "Iteration 1177, loss = 0.07788368\n",
            "Iteration 1178, loss = 0.07771787\n",
            "Iteration 1179, loss = 0.07755196\n",
            "Iteration 1180, loss = 0.07738620\n",
            "Iteration 1181, loss = 0.07722104\n",
            "Iteration 1182, loss = 0.07705677\n",
            "Iteration 1183, loss = 0.07689330\n",
            "Iteration 1184, loss = 0.07673035\n",
            "Iteration 1185, loss = 0.07656768\n",
            "Iteration 1186, loss = 0.07640523\n",
            "Iteration 1187, loss = 0.07624315\n",
            "Iteration 1188, loss = 0.07608161\n",
            "Iteration 1189, loss = 0.07592070\n",
            "Iteration 1190, loss = 0.07576042\n",
            "Iteration 1191, loss = 0.07560067\n",
            "Iteration 1192, loss = 0.07544135\n",
            "Iteration 1193, loss = 0.07528240\n",
            "Iteration 1194, loss = 0.07512383\n",
            "Iteration 1195, loss = 0.07496568\n",
            "Iteration 1196, loss = 0.07480800\n",
            "Iteration 1197, loss = 0.07465081\n",
            "Iteration 1198, loss = 0.07449414\n",
            "Iteration 1199, loss = 0.07433797\n",
            "Iteration 1200, loss = 0.07418227\n",
            "Iteration 1201, loss = 0.07402702\n",
            "Iteration 1202, loss = 0.07387221\n",
            "Iteration 1203, loss = 0.07371783\n",
            "Iteration 1204, loss = 0.07356387\n",
            "Iteration 1205, loss = 0.07341034\n",
            "Iteration 1206, loss = 0.07325724\n",
            "Iteration 1207, loss = 0.07310457\n",
            "Iteration 1208, loss = 0.07295234\n",
            "Iteration 1209, loss = 0.07280054\n",
            "Iteration 1210, loss = 0.07264918\n",
            "Iteration 1211, loss = 0.07249824\n",
            "Iteration 1212, loss = 0.07234774\n",
            "Iteration 1213, loss = 0.07219767\n",
            "Iteration 1214, loss = 0.07204802\n",
            "Iteration 1215, loss = 0.07189880\n",
            "Iteration 1216, loss = 0.07175000\n",
            "Iteration 1217, loss = 0.07160163\n",
            "Iteration 1218, loss = 0.07145370\n",
            "Iteration 1219, loss = 0.07130622\n",
            "Iteration 1220, loss = 0.07115922\n",
            "Iteration 1221, loss = 0.07101277\n",
            "Iteration 1222, loss = 0.07086695\n",
            "Iteration 1223, loss = 0.07072198\n",
            "Iteration 1224, loss = 0.07057820\n",
            "Iteration 1225, loss = 0.07043645\n",
            "Iteration 1226, loss = 0.07029788\n",
            "Iteration 1227, loss = 0.07016605\n",
            "Iteration 1228, loss = 0.07004454\n",
            "Iteration 1229, loss = 0.06994887\n",
            "Iteration 1230, loss = 0.06988401\n",
            "Iteration 1231, loss = 0.06991139\n",
            "Iteration 1232, loss = 0.06995998\n",
            "Iteration 1233, loss = 0.07017385\n",
            "Iteration 1234, loss = 0.06998088\n",
            "Iteration 1235, loss = 0.06968482\n",
            "Iteration 1236, loss = 0.06903046\n",
            "Iteration 1237, loss = 0.06873356\n",
            "Iteration 1238, loss = 0.06883662\n",
            "Iteration 1239, loss = 0.06888887\n",
            "Iteration 1240, loss = 0.06870631\n",
            "Iteration 1241, loss = 0.06825391\n",
            "Iteration 1242, loss = 0.06805995\n",
            "Iteration 1243, loss = 0.06811828\n",
            "Iteration 1244, loss = 0.06803089\n",
            "Iteration 1245, loss = 0.06776211\n",
            "Iteration 1246, loss = 0.06749793\n",
            "Iteration 1247, loss = 0.06743970\n",
            "Iteration 1248, loss = 0.06742124\n",
            "Iteration 1249, loss = 0.06721270\n",
            "Iteration 1250, loss = 0.06697202\n",
            "Iteration 1251, loss = 0.06685056\n",
            "Iteration 1252, loss = 0.06679390\n",
            "Iteration 1253, loss = 0.06666645\n",
            "Iteration 1254, loss = 0.06645314\n",
            "Iteration 1255, loss = 0.06630071\n",
            "Iteration 1256, loss = 0.06622056\n",
            "Iteration 1257, loss = 0.06610428\n",
            "Iteration 1258, loss = 0.06593285\n",
            "Iteration 1259, loss = 0.06577020\n",
            "Iteration 1260, loss = 0.06566207\n",
            "Iteration 1261, loss = 0.06555983\n",
            "Iteration 1262, loss = 0.06541083\n",
            "Iteration 1263, loss = 0.06525188\n",
            "Iteration 1264, loss = 0.06512380\n",
            "Iteration 1265, loss = 0.06501498\n",
            "Iteration 1266, loss = 0.06488916\n",
            "Iteration 1267, loss = 0.06474005\n",
            "Iteration 1268, loss = 0.06460073\n",
            "Iteration 1269, loss = 0.06448195\n",
            "Iteration 1270, loss = 0.06436294\n",
            "Iteration 1271, loss = 0.06422915\n",
            "Iteration 1272, loss = 0.06408850\n",
            "Iteration 1273, loss = 0.06395872\n",
            "Iteration 1274, loss = 0.06383863\n",
            "Iteration 1275, loss = 0.06371407\n",
            "Iteration 1276, loss = 0.06358076\n",
            "Iteration 1277, loss = 0.06344617\n",
            "Iteration 1278, loss = 0.06331878\n",
            "Iteration 1279, loss = 0.06319602\n",
            "Iteration 1280, loss = 0.06306991\n",
            "Iteration 1281, loss = 0.06293876\n",
            "Iteration 1282, loss = 0.06280693\n",
            "Iteration 1283, loss = 0.06267903\n",
            "Iteration 1284, loss = 0.06255399\n",
            "Iteration 1285, loss = 0.06242754\n",
            "Iteration 1286, loss = 0.06229801\n",
            "Iteration 1287, loss = 0.06216703\n",
            "Iteration 1288, loss = 0.06203746\n",
            "Iteration 1289, loss = 0.06190979\n",
            "Iteration 1290, loss = 0.06178221\n",
            "Iteration 1291, loss = 0.06165302\n",
            "Iteration 1292, loss = 0.06152190\n",
            "Iteration 1293, loss = 0.06139014\n",
            "Iteration 1294, loss = 0.06125876\n",
            "Iteration 1295, loss = 0.06112777\n",
            "Iteration 1296, loss = 0.06099636\n",
            "Iteration 1297, loss = 0.06086365\n",
            "Iteration 1298, loss = 0.06072943\n",
            "Iteration 1299, loss = 0.06059393\n",
            "Iteration 1300, loss = 0.06045767\n",
            "Iteration 1301, loss = 0.06032095\n",
            "Iteration 1302, loss = 0.06018380\n",
            "Iteration 1303, loss = 0.06004607\n",
            "Iteration 1304, loss = 0.05990758\n",
            "Iteration 1305, loss = 0.05976826\n",
            "Iteration 1306, loss = 0.05962808\n",
            "Iteration 1307, loss = 0.05948718\n",
            "Iteration 1308, loss = 0.05934565\n",
            "Iteration 1309, loss = 0.05920371\n",
            "Iteration 1310, loss = 0.05906150\n",
            "Iteration 1311, loss = 0.05891920\n",
            "Iteration 1312, loss = 0.05877687\n",
            "Iteration 1313, loss = 0.05863455\n",
            "Iteration 1314, loss = 0.05849225\n",
            "Iteration 1315, loss = 0.05835001\n",
            "Iteration 1316, loss = 0.05820791\n",
            "Iteration 1317, loss = 0.05806600\n",
            "Iteration 1318, loss = 0.05792437\n",
            "Iteration 1319, loss = 0.05778302\n",
            "Iteration 1320, loss = 0.05764201\n",
            "Iteration 1321, loss = 0.05750140\n",
            "Iteration 1322, loss = 0.05736135\n",
            "Iteration 1323, loss = 0.05722194\n",
            "Iteration 1324, loss = 0.05708343\n",
            "Iteration 1325, loss = 0.05694594\n",
            "Iteration 1326, loss = 0.05681015\n",
            "Iteration 1327, loss = 0.05667636\n",
            "Iteration 1328, loss = 0.05654670\n",
            "Iteration 1329, loss = 0.05642153\n",
            "Iteration 1330, loss = 0.05630779\n",
            "Iteration 1331, loss = 0.05620380\n",
            "Iteration 1332, loss = 0.05613267\n",
            "Iteration 1333, loss = 0.05607162\n",
            "Iteration 1334, loss = 0.05608805\n",
            "Iteration 1335, loss = 0.05603978\n",
            "Iteration 1336, loss = 0.05607170\n",
            "Iteration 1337, loss = 0.05580897\n",
            "Iteration 1338, loss = 0.05554328\n",
            "Iteration 1339, loss = 0.05516300\n",
            "Iteration 1340, loss = 0.05493025\n",
            "Iteration 1341, loss = 0.05486277\n",
            "Iteration 1342, loss = 0.05486035\n",
            "Iteration 1343, loss = 0.05483308\n",
            "Iteration 1344, loss = 0.05462236\n",
            "Iteration 1345, loss = 0.05438024\n",
            "Iteration 1346, loss = 0.05417563\n",
            "Iteration 1347, loss = 0.05407842\n",
            "Iteration 1348, loss = 0.05403736\n",
            "Iteration 1349, loss = 0.05394072\n",
            "Iteration 1350, loss = 0.05378781\n",
            "Iteration 1351, loss = 0.05359052\n",
            "Iteration 1352, loss = 0.05343859\n",
            "Iteration 1353, loss = 0.05334575\n",
            "Iteration 1354, loss = 0.05326646\n",
            "Iteration 1355, loss = 0.05316080\n",
            "Iteration 1356, loss = 0.05300643\n",
            "Iteration 1357, loss = 0.05285263\n",
            "Iteration 1358, loss = 0.05272791\n",
            "Iteration 1359, loss = 0.05263174\n",
            "Iteration 1360, loss = 0.05253795\n",
            "Iteration 1361, loss = 0.05241873\n",
            "Iteration 1362, loss = 0.05228445\n",
            "Iteration 1363, loss = 0.05215137\n",
            "Iteration 1364, loss = 0.05203644\n",
            "Iteration 1365, loss = 0.05193557\n",
            "Iteration 1366, loss = 0.05183243\n",
            "Iteration 1367, loss = 0.05171893\n",
            "Iteration 1368, loss = 0.05159512\n",
            "Iteration 1369, loss = 0.05147396\n",
            "Iteration 1370, loss = 0.05136223\n",
            "Iteration 1371, loss = 0.05125814\n",
            "Iteration 1372, loss = 0.05115444\n",
            "Iteration 1373, loss = 0.05104459\n",
            "Iteration 1374, loss = 0.05093039\n",
            "Iteration 1375, loss = 0.05081592\n",
            "Iteration 1376, loss = 0.05070595\n",
            "Iteration 1377, loss = 0.05060075\n",
            "Iteration 1378, loss = 0.05049717\n",
            "Iteration 1379, loss = 0.05039211\n",
            "Iteration 1380, loss = 0.05028408\n",
            "Iteration 1381, loss = 0.05017509\n",
            "Iteration 1382, loss = 0.05006730\n",
            "Iteration 1383, loss = 0.04996214\n",
            "Iteration 1384, loss = 0.04985913\n",
            "Iteration 1385, loss = 0.04975667\n",
            "Iteration 1386, loss = 0.04965356\n",
            "Iteration 1387, loss = 0.04954935\n",
            "Iteration 1388, loss = 0.04944492\n",
            "Iteration 1389, loss = 0.04934121\n",
            "Iteration 1390, loss = 0.04923884\n",
            "Iteration 1391, loss = 0.04913772\n",
            "Iteration 1392, loss = 0.04903726\n",
            "Iteration 1393, loss = 0.04893688\n",
            "Iteration 1394, loss = 0.04883625\n",
            "Iteration 1395, loss = 0.04873553\n",
            "Iteration 1396, loss = 0.04863505\n",
            "Iteration 1397, loss = 0.04853519\n",
            "Iteration 1398, loss = 0.04843608\n",
            "Iteration 1399, loss = 0.04833767\n",
            "Iteration 1400, loss = 0.04823974\n",
            "Iteration 1401, loss = 0.04814207\n",
            "Iteration 1402, loss = 0.04804455\n",
            "Iteration 1403, loss = 0.04794715\n",
            "Iteration 1404, loss = 0.04785000\n",
            "Iteration 1405, loss = 0.04775321\n",
            "Iteration 1406, loss = 0.04765686\n",
            "Iteration 1407, loss = 0.04756099\n",
            "Iteration 1408, loss = 0.04746557\n",
            "Iteration 1409, loss = 0.04737054\n",
            "Iteration 1410, loss = 0.04727583\n",
            "Iteration 1411, loss = 0.04718141\n",
            "Iteration 1412, loss = 0.04708724\n",
            "Iteration 1413, loss = 0.04699334\n",
            "Iteration 1414, loss = 0.04689972\n",
            "Iteration 1415, loss = 0.04680642\n",
            "Iteration 1416, loss = 0.04671346\n",
            "Iteration 1417, loss = 0.04662084\n",
            "Iteration 1418, loss = 0.04652857\n",
            "Iteration 1419, loss = 0.04643664\n",
            "Iteration 1420, loss = 0.04634504\n",
            "Iteration 1421, loss = 0.04625375\n",
            "Iteration 1422, loss = 0.04616276\n",
            "Iteration 1423, loss = 0.04607207\n",
            "Iteration 1424, loss = 0.04598167\n",
            "Iteration 1425, loss = 0.04589155\n",
            "Iteration 1426, loss = 0.04580173\n",
            "Iteration 1427, loss = 0.04571218\n",
            "Iteration 1428, loss = 0.04562293\n",
            "Iteration 1429, loss = 0.04553396\n",
            "Iteration 1430, loss = 0.04544528\n",
            "Iteration 1431, loss = 0.04535689\n",
            "Iteration 1432, loss = 0.04526878\n",
            "Iteration 1433, loss = 0.04518096\n",
            "Iteration 1434, loss = 0.04509343\n",
            "Iteration 1435, loss = 0.04500617\n",
            "Iteration 1436, loss = 0.04491919\n",
            "Iteration 1437, loss = 0.04483249\n",
            "Iteration 1438, loss = 0.04474607\n",
            "Iteration 1439, loss = 0.04465991\n",
            "Iteration 1440, loss = 0.04457404\n",
            "Iteration 1441, loss = 0.04448843\n",
            "Iteration 1442, loss = 0.04440310\n",
            "Iteration 1443, loss = 0.04431804\n",
            "Iteration 1444, loss = 0.04423325\n",
            "Iteration 1445, loss = 0.04414873\n",
            "Iteration 1446, loss = 0.04406449\n",
            "Iteration 1447, loss = 0.04398054\n",
            "Iteration 1448, loss = 0.04389689\n",
            "Iteration 1449, loss = 0.04381356\n",
            "Iteration 1450, loss = 0.04373058\n",
            "Iteration 1451, loss = 0.04364800\n",
            "Iteration 1452, loss = 0.04356594\n",
            "Iteration 1453, loss = 0.04348450\n",
            "Iteration 1454, loss = 0.04340403\n",
            "Iteration 1455, loss = 0.04332477\n",
            "Iteration 1456, loss = 0.04324780\n",
            "Iteration 1457, loss = 0.04317353\n",
            "Iteration 1458, loss = 0.04310553\n",
            "Iteration 1459, loss = 0.04304364\n",
            "Iteration 1460, loss = 0.04299966\n",
            "Iteration 1461, loss = 0.04296584\n",
            "Iteration 1462, loss = 0.04297917\n",
            "Iteration 1463, loss = 0.04298260\n",
            "Iteration 1464, loss = 0.04307117\n",
            "Iteration 1465, loss = 0.04301232\n",
            "Iteration 1466, loss = 0.04299629\n",
            "Iteration 1467, loss = 0.04270040\n",
            "Iteration 1468, loss = 0.04243652\n",
            "Iteration 1469, loss = 0.04221455\n",
            "Iteration 1470, loss = 0.04214153\n",
            "Iteration 1471, loss = 0.04217436\n",
            "Iteration 1472, loss = 0.04218083\n",
            "Iteration 1473, loss = 0.04213347\n",
            "Iteration 1474, loss = 0.04194461\n",
            "Iteration 1475, loss = 0.04176808\n",
            "Iteration 1476, loss = 0.04166156\n",
            "Iteration 1477, loss = 0.04163429\n",
            "Iteration 1478, loss = 0.04162659\n",
            "Iteration 1479, loss = 0.04155054\n",
            "Iteration 1480, loss = 0.04143289\n",
            "Iteration 1481, loss = 0.04129861\n",
            "Iteration 1482, loss = 0.04120929\n",
            "Iteration 1483, loss = 0.04116433\n",
            "Iteration 1484, loss = 0.04112068\n",
            "Iteration 1485, loss = 0.04105144\n",
            "Iteration 1486, loss = 0.04094502\n",
            "Iteration 1487, loss = 0.04084255\n",
            "Iteration 1488, loss = 0.04076535\n",
            "Iteration 1489, loss = 0.04070988\n",
            "Iteration 1490, loss = 0.04065436\n",
            "Iteration 1491, loss = 0.04057751\n",
            "Iteration 1492, loss = 0.04048852\n",
            "Iteration 1493, loss = 0.04040102\n",
            "Iteration 1494, loss = 0.04032821\n",
            "Iteration 1495, loss = 0.04026648\n",
            "Iteration 1496, loss = 0.04020244\n",
            "Iteration 1497, loss = 0.04012949\n",
            "Iteration 1498, loss = 0.04004824\n",
            "Iteration 1499, loss = 0.03996921\n",
            "Iteration 1500, loss = 0.03989782\n",
            "Iteration 1501, loss = 0.03983225\n",
            "Iteration 1502, loss = 0.03976636\n",
            "Iteration 1503, loss = 0.03969502\n",
            "Iteration 1504, loss = 0.03962008\n",
            "Iteration 1505, loss = 0.03954541\n",
            "Iteration 1506, loss = 0.03947464\n",
            "Iteration 1507, loss = 0.03940740\n",
            "Iteration 1508, loss = 0.03934057\n",
            "Iteration 1509, loss = 0.03927174\n",
            "Iteration 1510, loss = 0.03920041\n",
            "Iteration 1511, loss = 0.03912878\n",
            "Iteration 1512, loss = 0.03905877\n",
            "Iteration 1513, loss = 0.03899089\n",
            "Iteration 1514, loss = 0.03892404\n",
            "Iteration 1515, loss = 0.03885661\n",
            "Iteration 1516, loss = 0.03878807\n",
            "Iteration 1517, loss = 0.03871881\n",
            "Iteration 1518, loss = 0.03864998\n",
            "Iteration 1519, loss = 0.03858223\n",
            "Iteration 1520, loss = 0.03851548\n",
            "Iteration 1521, loss = 0.03844905\n",
            "Iteration 1522, loss = 0.03838229\n",
            "Iteration 1523, loss = 0.03831509\n",
            "Iteration 1524, loss = 0.03824776\n",
            "Iteration 1525, loss = 0.03818078\n",
            "Iteration 1526, loss = 0.03811441\n",
            "Iteration 1527, loss = 0.03804858\n",
            "Iteration 1528, loss = 0.03798299\n",
            "Iteration 1529, loss = 0.03791734\n",
            "Iteration 1530, loss = 0.03785157\n",
            "Iteration 1531, loss = 0.03778581\n",
            "Iteration 1532, loss = 0.03772027\n",
            "Iteration 1533, loss = 0.03765507\n",
            "Iteration 1534, loss = 0.03759023\n",
            "Iteration 1535, loss = 0.03752564\n",
            "Iteration 1536, loss = 0.03746115\n",
            "Iteration 1537, loss = 0.03739671\n",
            "Iteration 1538, loss = 0.03733232\n",
            "Iteration 1539, loss = 0.03726806\n",
            "Iteration 1540, loss = 0.03720402\n",
            "Iteration 1541, loss = 0.03714022\n",
            "Iteration 1542, loss = 0.03707666\n",
            "Iteration 1543, loss = 0.03701328\n",
            "Iteration 1544, loss = 0.03695003\n",
            "Iteration 1545, loss = 0.03688689\n",
            "Iteration 1546, loss = 0.03682386\n",
            "Iteration 1547, loss = 0.03676098\n",
            "Iteration 1548, loss = 0.03669827\n",
            "Iteration 1549, loss = 0.03663575\n",
            "Iteration 1550, loss = 0.03657342\n",
            "Iteration 1551, loss = 0.03651126\n",
            "Iteration 1552, loss = 0.03644925\n",
            "Iteration 1553, loss = 0.03638738\n",
            "Iteration 1554, loss = 0.03632564\n",
            "Iteration 1555, loss = 0.03626405\n",
            "Iteration 1556, loss = 0.03620260\n",
            "Iteration 1557, loss = 0.03614131\n",
            "Iteration 1558, loss = 0.03608019\n",
            "Iteration 1559, loss = 0.03601923\n",
            "Iteration 1560, loss = 0.03595842\n",
            "Iteration 1561, loss = 0.03589776\n",
            "Iteration 1562, loss = 0.03583725\n",
            "Iteration 1563, loss = 0.03577687\n",
            "Iteration 1564, loss = 0.03571664\n",
            "Iteration 1565, loss = 0.03565656\n",
            "Iteration 1566, loss = 0.03559661\n",
            "Iteration 1567, loss = 0.03553682\n",
            "Iteration 1568, loss = 0.03547717\n",
            "Iteration 1569, loss = 0.03541767\n",
            "Iteration 1570, loss = 0.03535832\n",
            "Iteration 1571, loss = 0.03529911\n",
            "Iteration 1572, loss = 0.03524004\n",
            "Iteration 1573, loss = 0.03518112\n",
            "Iteration 1574, loss = 0.03512233\n",
            "Iteration 1575, loss = 0.03506368\n",
            "Iteration 1576, loss = 0.03500517\n",
            "Iteration 1577, loss = 0.03494679\n",
            "Iteration 1578, loss = 0.03488856\n",
            "Iteration 1579, loss = 0.03483046\n",
            "Iteration 1580, loss = 0.03477251\n",
            "Iteration 1581, loss = 0.03471469\n",
            "Iteration 1582, loss = 0.03465701\n",
            "Iteration 1583, loss = 0.03459946\n",
            "Iteration 1584, loss = 0.03454205\n",
            "Iteration 1585, loss = 0.03448477\n",
            "Iteration 1586, loss = 0.03442763\n",
            "Iteration 1587, loss = 0.03437061\n",
            "Iteration 1588, loss = 0.03431374\n",
            "Iteration 1589, loss = 0.03425699\n",
            "Iteration 1590, loss = 0.03420037\n",
            "Iteration 1591, loss = 0.03414389\n",
            "Iteration 1592, loss = 0.03408753\n",
            "Iteration 1593, loss = 0.03403131\n",
            "Iteration 1594, loss = 0.03397522\n",
            "Iteration 1595, loss = 0.03391925\n",
            "Iteration 1596, loss = 0.03386341\n",
            "Iteration 1597, loss = 0.03380771\n",
            "Iteration 1598, loss = 0.03375212\n",
            "Iteration 1599, loss = 0.03369667\n",
            "Iteration 1600, loss = 0.03364134\n",
            "Iteration 1601, loss = 0.03358614\n",
            "Iteration 1602, loss = 0.03353106\n",
            "Iteration 1603, loss = 0.03347611\n",
            "Iteration 1604, loss = 0.03342128\n",
            "Iteration 1605, loss = 0.03336658\n",
            "Iteration 1606, loss = 0.03331200\n",
            "Iteration 1607, loss = 0.03325755\n",
            "Iteration 1608, loss = 0.03320321\n",
            "Iteration 1609, loss = 0.03314900\n",
            "Iteration 1610, loss = 0.03309491\n",
            "Iteration 1611, loss = 0.03304094\n",
            "Iteration 1612, loss = 0.03298709\n",
            "Iteration 1613, loss = 0.03293336\n",
            "Iteration 1614, loss = 0.03287976\n",
            "Iteration 1615, loss = 0.03282627\n",
            "Iteration 1616, loss = 0.03277290\n",
            "Iteration 1617, loss = 0.03271965\n",
            "Iteration 1618, loss = 0.03266652\n",
            "Iteration 1619, loss = 0.03261351\n",
            "Iteration 1620, loss = 0.03256061\n",
            "Iteration 1621, loss = 0.03250783\n",
            "Iteration 1622, loss = 0.03245517\n",
            "Iteration 1623, loss = 0.03240263\n",
            "Iteration 1624, loss = 0.03235021\n",
            "Iteration 1625, loss = 0.03229790\n",
            "Iteration 1626, loss = 0.03224572\n",
            "Iteration 1627, loss = 0.03219366\n",
            "Iteration 1628, loss = 0.03214172\n",
            "Iteration 1629, loss = 0.03208990\n",
            "Iteration 1630, loss = 0.03203823\n",
            "Iteration 1631, loss = 0.03198670\n",
            "Iteration 1632, loss = 0.03193533\n",
            "Iteration 1633, loss = 0.03188415\n",
            "Iteration 1634, loss = 0.03183320\n",
            "Iteration 1635, loss = 0.03178255\n",
            "Iteration 1636, loss = 0.03173227\n",
            "Iteration 1637, loss = 0.03168259\n",
            "Iteration 1638, loss = 0.03163365\n",
            "Iteration 1639, loss = 0.03158607\n",
            "Iteration 1640, loss = 0.03154006\n",
            "Iteration 1641, loss = 0.03149751\n",
            "Iteration 1642, loss = 0.03145830\n",
            "Iteration 1643, loss = 0.03142809\n",
            "Iteration 1644, loss = 0.03140383\n",
            "Iteration 1645, loss = 0.03140226\n",
            "Iteration 1646, loss = 0.03140368\n",
            "Iteration 1647, loss = 0.03145320\n",
            "Iteration 1648, loss = 0.03146437\n",
            "Iteration 1649, loss = 0.03153575\n",
            "Iteration 1650, loss = 0.03144744\n",
            "Iteration 1651, loss = 0.03137373\n",
            "Iteration 1652, loss = 0.03114030\n",
            "Iteration 1653, loss = 0.03095151\n",
            "Iteration 1654, loss = 0.03083285\n",
            "Iteration 1655, loss = 0.03081484\n",
            "Iteration 1656, loss = 0.03085388\n",
            "Iteration 1657, loss = 0.03085778\n",
            "Iteration 1658, loss = 0.03081987\n",
            "Iteration 1659, loss = 0.03069177\n",
            "Iteration 1660, loss = 0.03057056\n",
            "Iteration 1661, loss = 0.03049360\n",
            "Iteration 1662, loss = 0.03047279\n",
            "Iteration 1663, loss = 0.03047345\n",
            "Iteration 1664, loss = 0.03044030\n",
            "Iteration 1665, loss = 0.03037552\n",
            "Iteration 1666, loss = 0.03028342\n",
            "Iteration 1667, loss = 0.03020970\n",
            "Iteration 1668, loss = 0.03016805\n",
            "Iteration 1669, loss = 0.03014465\n",
            "Iteration 1670, loss = 0.03011573\n",
            "Iteration 1671, loss = 0.03006047\n",
            "Iteration 1672, loss = 0.02999335\n",
            "Iteration 1673, loss = 0.02992920\n",
            "Iteration 1674, loss = 0.02988188\n",
            "Iteration 1675, loss = 0.02984733\n",
            "Iteration 1676, loss = 0.02981099\n",
            "Iteration 1677, loss = 0.02976488\n",
            "Iteration 1678, loss = 0.02970783\n",
            "Iteration 1679, loss = 0.02965150\n",
            "Iteration 1680, loss = 0.02960305\n",
            "Iteration 1681, loss = 0.02956236\n",
            "Iteration 1682, loss = 0.02952308\n",
            "Iteration 1683, loss = 0.02947840\n",
            "Iteration 1684, loss = 0.02942863\n",
            "Iteration 1685, loss = 0.02937718\n",
            "Iteration 1686, loss = 0.02932920\n",
            "Iteration 1687, loss = 0.02928573\n",
            "Iteration 1688, loss = 0.02924403\n",
            "Iteration 1689, loss = 0.02920074\n",
            "Iteration 1690, loss = 0.02915421\n",
            "Iteration 1691, loss = 0.02910630\n",
            "Iteration 1692, loss = 0.02905937\n",
            "Iteration 1693, loss = 0.02901482\n",
            "Iteration 1694, loss = 0.02897193\n",
            "Iteration 1695, loss = 0.02892883\n",
            "Iteration 1696, loss = 0.02888441\n",
            "Iteration 1697, loss = 0.02883873\n",
            "Iteration 1698, loss = 0.02879309\n",
            "Iteration 1699, loss = 0.02874848\n",
            "Iteration 1700, loss = 0.02870502\n",
            "Iteration 1701, loss = 0.02866202\n",
            "Iteration 1702, loss = 0.02861860\n",
            "Iteration 1703, loss = 0.02857452\n",
            "Iteration 1704, loss = 0.02853010\n",
            "Iteration 1705, loss = 0.02848599\n",
            "Iteration 1706, loss = 0.02844252\n",
            "Iteration 1707, loss = 0.02839957\n",
            "Iteration 1708, loss = 0.02835672\n",
            "Iteration 1709, loss = 0.02831364\n",
            "Iteration 1710, loss = 0.02827031\n",
            "Iteration 1711, loss = 0.02822695\n",
            "Iteration 1712, loss = 0.02818385\n",
            "Iteration 1713, loss = 0.02814111\n",
            "Iteration 1714, loss = 0.02809862\n",
            "Iteration 1715, loss = 0.02805620\n",
            "Iteration 1716, loss = 0.02801370\n",
            "Iteration 1717, loss = 0.02797114\n",
            "Iteration 1718, loss = 0.02792862\n",
            "Iteration 1719, loss = 0.02788629\n",
            "Iteration 1720, loss = 0.02784417\n",
            "Iteration 1721, loss = 0.02780222\n",
            "Iteration 1722, loss = 0.02776035\n",
            "Iteration 1723, loss = 0.02771848\n",
            "Iteration 1724, loss = 0.02767663\n",
            "Iteration 1725, loss = 0.02763485\n",
            "Iteration 1726, loss = 0.02759319\n",
            "Iteration 1727, loss = 0.02755169\n",
            "Iteration 1728, loss = 0.02751031\n",
            "Iteration 1729, loss = 0.02746902\n",
            "Iteration 1730, loss = 0.02742778\n",
            "Iteration 1731, loss = 0.02738660\n",
            "Iteration 1732, loss = 0.02734548\n",
            "Iteration 1733, loss = 0.02730447\n",
            "Iteration 1734, loss = 0.02726357\n",
            "Iteration 1735, loss = 0.02722279\n",
            "Iteration 1736, loss = 0.02718209\n",
            "Iteration 1737, loss = 0.02714147\n",
            "Iteration 1738, loss = 0.02710092\n",
            "Iteration 1739, loss = 0.02706044\n",
            "Iteration 1740, loss = 0.02702005\n",
            "Iteration 1741, loss = 0.02697976\n",
            "Iteration 1742, loss = 0.02693956\n",
            "Iteration 1743, loss = 0.02689946\n",
            "Iteration 1744, loss = 0.02685944\n",
            "Iteration 1745, loss = 0.02681950\n",
            "Iteration 1746, loss = 0.02677964\n",
            "Iteration 1747, loss = 0.02673985\n",
            "Iteration 1748, loss = 0.02670016\n",
            "Iteration 1749, loss = 0.02666055\n",
            "Iteration 1750, loss = 0.02662104\n",
            "Iteration 1751, loss = 0.02658161\n",
            "Iteration 1752, loss = 0.02654226\n",
            "Iteration 1753, loss = 0.02650299\n",
            "Iteration 1754, loss = 0.02646380\n",
            "Iteration 1755, loss = 0.02642469\n",
            "Iteration 1756, loss = 0.02638567\n",
            "Iteration 1757, loss = 0.02634673\n",
            "Iteration 1758, loss = 0.02630788\n",
            "Iteration 1759, loss = 0.02626911\n",
            "Iteration 1760, loss = 0.02623042\n",
            "Iteration 1761, loss = 0.02619181\n",
            "Iteration 1762, loss = 0.02615328\n",
            "Iteration 1763, loss = 0.02611484\n",
            "Iteration 1764, loss = 0.02607647\n",
            "Iteration 1765, loss = 0.02603818\n",
            "Iteration 1766, loss = 0.02599998\n",
            "Iteration 1767, loss = 0.02596186\n",
            "Iteration 1768, loss = 0.02592382\n",
            "Iteration 1769, loss = 0.02588585\n",
            "Iteration 1770, loss = 0.02584797\n",
            "Iteration 1771, loss = 0.02581016\n",
            "Iteration 1772, loss = 0.02577243\n",
            "Iteration 1773, loss = 0.02573479\n",
            "Iteration 1774, loss = 0.02569722\n",
            "Iteration 1775, loss = 0.02565973\n",
            "Iteration 1776, loss = 0.02562232\n",
            "Iteration 1777, loss = 0.02558499\n",
            "Iteration 1778, loss = 0.02554773\n",
            "Iteration 1779, loss = 0.02551055\n",
            "Iteration 1780, loss = 0.02547345\n",
            "Iteration 1781, loss = 0.02543643\n",
            "Iteration 1782, loss = 0.02539948\n",
            "Iteration 1783, loss = 0.02536261\n",
            "Iteration 1784, loss = 0.02532582\n",
            "Iteration 1785, loss = 0.02528910\n",
            "Iteration 1786, loss = 0.02525246\n",
            "Iteration 1787, loss = 0.02521589\n",
            "Iteration 1788, loss = 0.02517940\n",
            "Iteration 1789, loss = 0.02514298\n",
            "Iteration 1790, loss = 0.02510664\n",
            "Iteration 1791, loss = 0.02507038\n",
            "Iteration 1792, loss = 0.02503419\n",
            "Iteration 1793, loss = 0.02499807\n",
            "Iteration 1794, loss = 0.02496203\n",
            "Iteration 1795, loss = 0.02492606\n",
            "Iteration 1796, loss = 0.02489017\n",
            "Iteration 1797, loss = 0.02485435\n",
            "Iteration 1798, loss = 0.02481860\n",
            "Iteration 1799, loss = 0.02478292\n",
            "Iteration 1800, loss = 0.02474732\n",
            "Iteration 1801, loss = 0.02471179\n",
            "Iteration 1802, loss = 0.02467634\n",
            "Iteration 1803, loss = 0.02464095\n",
            "Iteration 1804, loss = 0.02460564\n",
            "Iteration 1805, loss = 0.02457040\n",
            "Iteration 1806, loss = 0.02453523\n",
            "Iteration 1807, loss = 0.02450013\n",
            "Iteration 1808, loss = 0.02446511\n",
            "Iteration 1809, loss = 0.02443015\n",
            "Iteration 1810, loss = 0.02439527\n",
            "Iteration 1811, loss = 0.02436045\n",
            "Iteration 1812, loss = 0.02432571\n",
            "Iteration 1813, loss = 0.02429104\n",
            "Iteration 1814, loss = 0.02425643\n",
            "Iteration 1815, loss = 0.02422190\n",
            "Iteration 1816, loss = 0.02418743\n",
            "Iteration 1817, loss = 0.02415304\n",
            "Iteration 1818, loss = 0.02411871\n",
            "Iteration 1819, loss = 0.02408445\n",
            "Iteration 1820, loss = 0.02405026\n",
            "Iteration 1821, loss = 0.02401614\n",
            "Iteration 1822, loss = 0.02398209\n",
            "Iteration 1823, loss = 0.02394811\n",
            "Iteration 1824, loss = 0.02391419\n",
            "Iteration 1825, loss = 0.02388034\n",
            "Iteration 1826, loss = 0.02384656\n",
            "Iteration 1827, loss = 0.02381284\n",
            "Iteration 1828, loss = 0.02377920\n",
            "Iteration 1829, loss = 0.02374561\n",
            "Iteration 1830, loss = 0.02371210\n",
            "Iteration 1831, loss = 0.02367865\n",
            "Iteration 1832, loss = 0.02364527\n",
            "Iteration 1833, loss = 0.02361196\n",
            "Iteration 1834, loss = 0.02357871\n",
            "Iteration 1835, loss = 0.02354552\n",
            "Iteration 1836, loss = 0.02351240\n",
            "Iteration 1837, loss = 0.02347935\n",
            "Iteration 1838, loss = 0.02344636\n",
            "Iteration 1839, loss = 0.02341344\n",
            "Iteration 1840, loss = 0.02338058\n",
            "Iteration 1841, loss = 0.02334778\n",
            "Iteration 1842, loss = 0.02331505\n",
            "Iteration 1843, loss = 0.02328238\n",
            "Iteration 1844, loss = 0.02324978\n",
            "Iteration 1845, loss = 0.02321724\n",
            "Iteration 1846, loss = 0.02318477\n",
            "Iteration 1847, loss = 0.02315236\n",
            "Iteration 1848, loss = 0.02312001\n",
            "Iteration 1849, loss = 0.02308772\n",
            "Iteration 1850, loss = 0.02305550\n",
            "Iteration 1851, loss = 0.02302333\n",
            "Iteration 1852, loss = 0.02299124\n",
            "Iteration 1853, loss = 0.02295920\n",
            "Iteration 1854, loss = 0.02292723\n",
            "Iteration 1855, loss = 0.02289531\n",
            "Iteration 1856, loss = 0.02286346\n",
            "Iteration 1857, loss = 0.02283167\n",
            "Iteration 1858, loss = 0.02279994\n",
            "Iteration 1859, loss = 0.02276828\n",
            "Iteration 1860, loss = 0.02273667\n",
            "Iteration 1861, loss = 0.02270512\n",
            "Iteration 1862, loss = 0.02267364\n",
            "Iteration 1863, loss = 0.02264221\n",
            "Iteration 1864, loss = 0.02261085\n",
            "Iteration 1865, loss = 0.02257954\n",
            "Iteration 1866, loss = 0.02254830\n",
            "Iteration 1867, loss = 0.02251711\n",
            "Iteration 1868, loss = 0.02248599\n",
            "Iteration 1869, loss = 0.02245492\n",
            "Iteration 1870, loss = 0.02242391\n",
            "Iteration 1871, loss = 0.02239296\n",
            "Iteration 1872, loss = 0.02236207\n",
            "Iteration 1873, loss = 0.02233124\n",
            "Iteration 1874, loss = 0.02230047\n",
            "Iteration 1875, loss = 0.02226975\n",
            "Iteration 1876, loss = 0.02223910\n",
            "Iteration 1877, loss = 0.02220850\n",
            "Iteration 1878, loss = 0.02217796\n",
            "Iteration 1879, loss = 0.02214747\n",
            "Iteration 1880, loss = 0.02211705\n",
            "Iteration 1881, loss = 0.02208668\n",
            "Iteration 1882, loss = 0.02205637\n",
            "Iteration 1883, loss = 0.02202611\n",
            "Iteration 1884, loss = 0.02199591\n",
            "Iteration 1885, loss = 0.02196577\n",
            "Iteration 1886, loss = 0.02193569\n",
            "Iteration 1887, loss = 0.02190566\n",
            "Iteration 1888, loss = 0.02187568\n",
            "Iteration 1889, loss = 0.02184577\n",
            "Iteration 1890, loss = 0.02181591\n",
            "Iteration 1891, loss = 0.02178610\n",
            "Iteration 1892, loss = 0.02175635\n",
            "Iteration 1893, loss = 0.02172666\n",
            "Iteration 1894, loss = 0.02169702\n",
            "Iteration 1895, loss = 0.02166743\n",
            "Iteration 1896, loss = 0.02163790\n",
            "Iteration 1897, loss = 0.02160842\n",
            "Iteration 1898, loss = 0.02157900\n",
            "Iteration 1899, loss = 0.02154964\n",
            "Iteration 1900, loss = 0.02152032\n",
            "Iteration 1901, loss = 0.02149106\n",
            "Iteration 1902, loss = 0.02146186\n",
            "Iteration 1903, loss = 0.02143271\n",
            "Iteration 1904, loss = 0.02140361\n",
            "Iteration 1905, loss = 0.02137457\n",
            "Iteration 1906, loss = 0.02134557\n",
            "Iteration 1907, loss = 0.02131664\n",
            "Iteration 1908, loss = 0.02128775\n",
            "Iteration 1909, loss = 0.02125892\n",
            "Iteration 1910, loss = 0.02123014\n",
            "Iteration 1911, loss = 0.02120141\n",
            "Iteration 1912, loss = 0.02117274\n",
            "Iteration 1913, loss = 0.02114411\n",
            "Iteration 1914, loss = 0.02111554\n",
            "Iteration 1915, loss = 0.02108702\n",
            "Iteration 1916, loss = 0.02105856\n",
            "Iteration 1917, loss = 0.02103014\n",
            "Iteration 1918, loss = 0.02100177\n",
            "Iteration 1919, loss = 0.02097346\n",
            "Iteration 1920, loss = 0.02094520\n",
            "Iteration 1921, loss = 0.02091699\n",
            "Iteration 1922, loss = 0.02088883\n",
            "Iteration 1923, loss = 0.02086072\n",
            "Iteration 1924, loss = 0.02083266\n",
            "Iteration 1925, loss = 0.02080465\n",
            "Iteration 1926, loss = 0.02077669\n",
            "Iteration 1927, loss = 0.02074878\n",
            "Iteration 1928, loss = 0.02072092\n",
            "Iteration 1929, loss = 0.02069311\n",
            "Iteration 1930, loss = 0.02066536\n",
            "Iteration 1931, loss = 0.02063765\n",
            "Iteration 1932, loss = 0.02060998\n",
            "Iteration 1933, loss = 0.02058237\n",
            "Iteration 1934, loss = 0.02055481\n",
            "Iteration 1935, loss = 0.02052730\n",
            "Iteration 1936, loss = 0.02049983\n",
            "Iteration 1937, loss = 0.02047242\n",
            "Iteration 1938, loss = 0.02044505\n",
            "Iteration 1939, loss = 0.02041773\n",
            "Iteration 1940, loss = 0.02039046\n",
            "Iteration 1941, loss = 0.02036324\n",
            "Iteration 1942, loss = 0.02033606\n",
            "Iteration 1943, loss = 0.02030894\n",
            "Iteration 1944, loss = 0.02028186\n",
            "Iteration 1945, loss = 0.02025483\n",
            "Iteration 1946, loss = 0.02022784\n",
            "Iteration 1947, loss = 0.02020091\n",
            "Iteration 1948, loss = 0.02017402\n",
            "Iteration 1949, loss = 0.02014718\n",
            "Iteration 1950, loss = 0.02012038\n",
            "Iteration 1951, loss = 0.02009363\n",
            "Iteration 1952, loss = 0.02006693\n",
            "Iteration 1953, loss = 0.02004027\n",
            "Iteration 1954, loss = 0.02001366\n",
            "Iteration 1955, loss = 0.01998710\n",
            "Iteration 1956, loss = 0.01996058\n",
            "Iteration 1957, loss = 0.01993411\n",
            "Iteration 1958, loss = 0.01990769\n",
            "Iteration 1959, loss = 0.01988131\n",
            "Iteration 1960, loss = 0.01985497\n",
            "Iteration 1961, loss = 0.01982868\n",
            "Iteration 1962, loss = 0.01980244\n",
            "Iteration 1963, loss = 0.01977624\n",
            "Iteration 1964, loss = 0.01975009\n",
            "Iteration 1965, loss = 0.01972398\n",
            "Iteration 1966, loss = 0.01969792\n",
            "Iteration 1967, loss = 0.01967190\n",
            "Iteration 1968, loss = 0.01964592\n",
            "Iteration 1969, loss = 0.01961999\n",
            "Iteration 1970, loss = 0.01959411\n",
            "Iteration 1971, loss = 0.01956827\n",
            "Iteration 1972, loss = 0.01954247\n",
            "Iteration 1973, loss = 0.01951672\n",
            "Iteration 1974, loss = 0.01949101\n",
            "Iteration 1975, loss = 0.01946534\n",
            "Iteration 1976, loss = 0.01943972\n",
            "Iteration 1977, loss = 0.01941414\n",
            "Iteration 1978, loss = 0.01938860\n",
            "Iteration 1979, loss = 0.01936311\n",
            "Iteration 1980, loss = 0.01933766\n",
            "Iteration 1981, loss = 0.01931225\n",
            "Iteration 1982, loss = 0.01928689\n",
            "Iteration 1983, loss = 0.01926157\n",
            "Iteration 1984, loss = 0.01923629\n",
            "Iteration 1985, loss = 0.01921105\n",
            "Iteration 1986, loss = 0.01918585\n",
            "Iteration 1987, loss = 0.01916070\n",
            "Iteration 1988, loss = 0.01913559\n",
            "Iteration 1989, loss = 0.01911052\n",
            "Iteration 1990, loss = 0.01908550\n",
            "Iteration 1991, loss = 0.01906051\n",
            "Iteration 1992, loss = 0.01903557\n",
            "Iteration 1993, loss = 0.01901067\n",
            "Iteration 1994, loss = 0.01898581\n",
            "Iteration 1995, loss = 0.01896099\n",
            "Iteration 1996, loss = 0.01893621\n",
            "Iteration 1997, loss = 0.01891147\n",
            "Iteration 1998, loss = 0.01888678\n",
            "Iteration 1999, loss = 0.01886212\n",
            "Iteration 2000, loss = 0.01883750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
              "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(8, 8), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=2000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=1e-06, validation_fraction=0.1, verbose=True,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adSet8VkNt4U",
        "outputId": "fe6153da-025d-46c5-d929-10b418f52f89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "network.classes_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c9uGEYZNxy3",
        "outputId": "5f4d92a1-1622-4bb0-dcd9-bfc26bdac740",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "network.coefs_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 7.80909175e-02,  1.26023939e-02, -1.22351983e-01,\n",
              "         -2.81106236e-02, -4.74121400e-02, -7.09323806e-02,\n",
              "          2.59965500e-03, -1.29286834e-01],\n",
              "        [-5.72305667e-02,  1.54892612e-01,  7.47107758e-03,\n",
              "          5.22948242e-03,  2.77222416e-02, -3.82502588e-02,\n",
              "          3.33807760e-02,  1.04007645e-02],\n",
              "        [-9.74594921e-02, -3.39986855e-05, -4.30542231e-02,\n",
              "         -1.21121754e-01,  8.63093953e-02, -3.88248789e-04,\n",
              "          6.95120261e-02,  2.61539087e-04],\n",
              "        [-1.64339756e-01, -1.49031621e-03, -1.25588473e-01,\n",
              "          1.31824506e-01,  1.20010172e-04,  3.26299639e-02,\n",
              "         -8.34759388e-02, -8.49983563e-02],\n",
              "        [ 2.00253496e-02,  8.96859296e-03,  2.94730034e-02,\n",
              "          2.37171327e-02,  1.69036765e-01,  4.60926875e-03,\n",
              "         -1.31523883e-02, -1.10318648e-01],\n",
              "        [-4.98707154e-02, -1.63869814e-01,  1.23496815e-01,\n",
              "         -6.57252794e-03,  4.51559411e-04, -6.22491037e-02,\n",
              "          6.45589379e-02,  3.95610052e-02],\n",
              "        [-1.19901345e-01, -1.10681950e-01, -3.06420699e-02,\n",
              "         -1.84307489e-02,  1.83751340e-02,  1.48713372e-01,\n",
              "         -2.90049632e-03,  3.22324040e-03],\n",
              "        [ 1.22760615e-01,  2.71502860e-04, -5.72985118e-02,\n",
              "         -5.92891576e-03, -1.03262937e-02,  1.89658690e-02,\n",
              "         -1.59418331e-02, -9.01091977e-03],\n",
              "        [ 1.46290710e-03, -3.11887336e-03, -1.29881127e-03,\n",
              "         -7.64286540e-02,  1.58188247e-01,  1.38308065e-01,\n",
              "          1.14742380e-02,  1.41655256e-01],\n",
              "        [ 1.65371081e-01, -9.23350765e-03, -1.56234378e-01,\n",
              "          1.79477227e-02,  7.89652859e-03,  9.84822066e-02,\n",
              "         -1.32232444e-01, -9.20219976e-02],\n",
              "        [ 1.07559810e-01, -1.54157700e-02,  6.55340311e-02,\n",
              "          4.72926770e-02,  1.70062866e-03,  6.84302319e-02,\n",
              "         -5.98746087e-02,  8.57028902e-02],\n",
              "        [-3.36470218e-02,  6.08566427e-02, -7.85056864e-03,\n",
              "         -3.71205974e-02,  1.05646940e-01, -1.51295050e-01,\n",
              "          4.28973460e-03,  9.65890598e-02],\n",
              "        [ 5.33576377e-02,  4.60578548e-02, -4.55961275e-02,\n",
              "         -7.27102805e-02,  1.02178530e-01, -5.09537013e-02,\n",
              "         -4.75599014e-02,  9.22385716e-02]]),\n",
              " array([[-3.87423544e-01,  7.59947979e-02,  9.86121024e-02,\n",
              "          3.20397320e-01,  1.68642715e-01,  2.28803381e-01,\n",
              "         -3.59997298e-01, -1.06372607e-01],\n",
              "        [-1.24545191e-01, -1.87882904e-01, -1.80674473e-01,\n",
              "         -2.54753306e-02,  1.57851558e-01,  1.02935833e-02,\n",
              "         -2.45687716e-01, -1.78771547e-01],\n",
              "        [ 3.60946022e-02,  1.66120619e-01, -8.50461349e-03,\n",
              "          1.88392506e-03,  1.36560077e-01, -1.97642403e-02,\n",
              "          4.06199490e-02,  1.11590692e-01],\n",
              "        [-8.32274320e-02,  7.85910675e-02, -4.86287062e-02,\n",
              "          1.53478354e-01,  6.32426560e-04,  1.02307308e-02,\n",
              "          2.03939866e-04, -5.42994539e-03],\n",
              "        [-4.80572246e-02, -3.31942441e-01,  2.52154030e-01,\n",
              "          1.04018211e-01,  2.94435266e-01,  2.25933632e-01,\n",
              "          2.42455526e-01, -3.15477534e-01],\n",
              "        [-1.30881668e-01,  3.40670942e-02, -1.73350955e-01,\n",
              "         -1.67683706e-01,  9.14615763e-02,  1.57569563e-01,\n",
              "         -2.68953121e-02, -1.27176993e-01],\n",
              "        [ 7.98836039e-02, -3.65442539e-02, -2.01947811e-01,\n",
              "          9.98376796e-02, -2.01771263e-02,  4.03189386e-02,\n",
              "          1.00004636e-02,  1.10171505e-01],\n",
              "        [-1.07572206e-01, -1.25429376e-01, -1.25507961e-02,\n",
              "          4.52184836e-01, -7.15227601e-02, -5.30907599e-02,\n",
              "         -9.37918812e-02,  8.76293215e-02]]),\n",
              " array([[-0.41942652, -0.18537136,  0.03121165],\n",
              "        [ 0.27925797, -0.1403593 ,  0.22438287],\n",
              "        [ 0.01630367,  0.25219355,  0.13481427],\n",
              "        [-0.26061555,  0.21544243, -0.25880634],\n",
              "        [ 0.16399858, -0.23618723, -0.34573605],\n",
              "        [-0.27601978, -0.04860025, -0.17450142],\n",
              "        [ 0.26276971, -0.09518278, -0.02012296],\n",
              "        [ 0.07310097,  0.08608856,  0.07922931]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAdAAg7AN3Hm",
        "outputId": "691571c1-3778-4d56-a76a-e0907c8170da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "network.intercepts_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-0.13015379, -0.1248917 ,  0.08316125, -0.26492498,  0.0668621 ,\n",
              "         0.16755264,  0.20897855,  0.17413407]),\n",
              " array([ 0.23655067,  0.12147355, -0.3268484 ,  0.05565871, -0.17167014,\n",
              "        -0.20219906,  0.10761383, -0.15229972]),\n",
              " array([0.06876118, 0.15625947, 0.14219806])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5_1ACCZOIhJ",
        "outputId": "2157bbe7-77eb-4889-8e30-0eb6ede340af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "network.n_layers_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHDUyZqwOM7a",
        "outputId": "8063e1d9-d487-43ff-ac4f-3d204ec03373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "network.n_outputs_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10D1sM9IOTGG",
        "outputId": "45975015-2fa2-4bc0-fd60-4fecc35a2a40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "network.out_activation_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'softmax'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTaTfbXNmUrn"
      },
      "source": [
        "## Neural network (evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVsg-V0pOy5_",
        "outputId": "1f09745f-91d2-4057-b20e-975eb5edc27e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJmwyvk7O5kp",
        "outputId": "e85348a0-80c2-4850-b9cf-90fe24f33f8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "predictions = network.predict(X_test)\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 2, 1, 2, 1, 0, 0,\n",
              "       0, 0, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjt0xKZDPNlw",
        "outputId": "fa4f6e10-36ce-4f49-8315-9e6ca2b55ebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 2, 0, 2, 1, 2, 1, 0, 0,\n",
              "       0, 0, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkCZGjnVPdU3",
        "outputId": "577d2017-3fc2-4f6d-e61c-6e9f65102e78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "accuracy_score(y_test, predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9166666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSqMoc33Pvkz",
        "outputId": "acc83284-a39e-4cf2-9133-0cc315f4a21a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "cm = confusion_matrix(y_test, predictions)\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[13,  1,  0],\n",
              "       [ 1, 10,  1],\n",
              "       [ 0,  0, 10]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEMJsIY9QBh6",
        "outputId": "c7075766-1754-472e-8d77-aa312b9df948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "!pip install yellowbrick --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting yellowbrick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/95/a14e4fdfb8b1c8753bbe74a626e910a98219ef9c87c6763585bbd30d84cf/yellowbrick-1.1-py3-none-any.whl (263kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from yellowbrick) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from yellowbrick) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from yellowbrick) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from yellowbrick) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10.0 in /usr/local/lib/python3.6/dist-packages (from yellowbrick) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->yellowbrick) (0.15.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10.0->yellowbrick) (1.12.0)\n",
            "Installing collected packages: yellowbrick\n",
            "  Found existing installation: yellowbrick 0.9.1\n",
            "    Uninstalling yellowbrick-0.9.1:\n",
            "      Successfully uninstalled yellowbrick-0.9.1\n",
            "Successfully installed yellowbrick-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tP25AS-QIcp",
        "outputId": "659b21b8-3a25-4a7a-8d1c-260eab9904b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        }
      },
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "confusion_matrix = ConfusionMatrix(network, classes = wine.target_names)\n",
        "confusion_matrix.fit(X_train, y_train)\n",
        "confusion_matrix.score(X_test, y_test)\n",
        "confusion_matrix.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc26c5fe7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nkf_QRkpmZ7u"
      },
      "source": [
        "## Neural network (classification)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwunTr7AQ7vI",
        "outputId": "d5e00f21-6641-4926-ea37-a67fbbe72e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "X_test[0], y_test[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.35e+01, 1.81e+00, 2.61e+00, 2.00e+01, 9.60e+01, 2.53e+00,\n",
              "        2.61e+00, 2.80e-01, 1.66e+00, 3.52e+00, 1.12e+00, 3.82e+00,\n",
              "        8.45e+02]), 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGUxWMJMREcK",
        "outputId": "c7477033-9cf2-4d85-9f05-e2baf3d53b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzFuBVn_RLGT",
        "outputId": "b0d29807-289c-4cef-8aac-1438ecb78ef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new = X_test[0].reshape(1, -1)\n",
        "new.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzfNwQSMRUNX",
        "outputId": "0233366f-7458-49e7-e65c-d264d9210cd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.35e+01, 1.81e+00, 2.61e+00, 2.00e+01, 9.60e+01, 2.53e+00,\n",
              "        2.61e+00, 2.80e-01, 1.66e+00, 3.52e+00, 1.12e+00, 3.82e+00,\n",
              "        8.45e+02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSxnsC4yRZn0",
        "outputId": "a8483aaf-d9cc-45a7-ff62-80865f850503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "network.predict(new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2xAgLwmRfDB",
        "outputId": "6f38393a-323d-4d79-b98a-6ddf020dbdea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "wine.target_names[network.predict(new)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['class_1'], dtype='<U7')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    }
  ]
}